{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":88742,"databundleVersionId":10173359,"sourceType":"competition"}],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Data preprocessing and feature engineering is applied for the following notebooks.\n\nimport numpy as np  # Linear algebra operations\nimport pandas as pd  # Data processing, CSV file I/O (e.g., pd.read_csv)\nimport gc  # Garbage collection for memory management\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-11T06:10:07.087904Z","iopub.execute_input":"2025-02-11T06:10:07.088302Z","iopub.status.idle":"2025-02-11T06:10:07.520204Z","shell.execute_reply.started":"2025-02-11T06:10:07.088268Z","shell.execute_reply":"2025-02-11T06:10:07.518827Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Load datasets\ncalendar = pd.read_csv('/kaggle/input/rohlik-sales-forecasting-challenge-v2/calendar.csv')\ninventory = pd.read_csv('/kaggle/input/rohlik-sales-forecasting-challenge-v2/inventory.csv')\nsales_train = pd.read_csv('/kaggle/input/rohlik-sales-forecasting-challenge-v2/sales_train.csv')\nsales_test = pd.read_csv('/kaggle/input/rohlik-sales-forecasting-challenge-v2/sales_test.csv')\nsolution = pd.read_csv('/kaggle/input/rohlik-sales-forecasting-challenge-v2/solution.csv')\ntest_weights= pd.read_csv('/kaggle/input/rohlik-sales-forecasting-challenge-v2/test_weights.csv')\n\n\n# Add an empty (NaN) target column to test set since it is missing\nsales_test['sales'] = np.nan\n\n# Combine train and test datasets to ensure consistent processing\ncombined_df = pd.concat([sales_train, sales_test], ignore_index=True)\n\n# Convert date columns to datetime format for better handling\nsales_train['date'] = pd.to_datetime(sales_train['date'])\nsales_test['date'] = pd.to_datetime(sales_test['date'])\n\n# Compute the minimum and maximum dates for train and test sets\ntrain_min_date = sales_train['date'].min()\ntrain_max_date = sales_train['date'].max()\ntest_min_date = sales_test['date'].min()\ntest_max_date = sales_test['date'].max()\n\n# Sonuçları yazdırma\nprint(\"Sales Train - En Küçük Tarih:\", train_min_date)\nprint(\"Sales Train - En Büyük Tarih:\", train_max_date)\nprint(\"Sales Test - En Küçük Tarih:\", test_min_date)\nprint(\"Sales Test - En Büyük Tarih:\", test_max_date)\n   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T06:10:07.521812Z","iopub.execute_input":"2025-02-11T06:10:07.522505Z","iopub.status.idle":"2025-02-11T06:10:16.661990Z","shell.execute_reply.started":"2025-02-11T06:10:07.522423Z","shell.execute_reply":"2025-02-11T06:10:16.660953Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **DATA QUALITY**","metadata":{}},{"cell_type":"markdown","source":"**Missing Holiday Handling**","metadata":{}},{"cell_type":"code","source":"#Take all holidays\nfixed_holidays = {\n    'Germany': [\n        ('01-01', 'New Year'),\n        ('05-01', 'Labour Day'),\n        ('10-03', 'German Unity Day'),\n        ('12-24', 'Christmas Eve'),\n        ('12-25', 'Christmas Day'),\n        ('12-26', 'Boxing Day')\n    ],\n    'Czechia': [\n        ('01-01', 'New Year'),\n        ('05-01', 'Labour Day'),\n        ('09-28', 'St. Wenceslas Day'),\n        ('10-28', 'Czech Independence Day'),\n        ('12-24', 'Christmas Eve'),\n        ('12-25', 'Christmas Day'),\n        ('12-26', 'Boxing Day')\n    ],\n    'Hungary': [\n        ('01-01', 'New Year'),\n        ('03-15', 'Revolution Day'),\n        ('05-01', 'Labour Day'),\n        ('08-20', 'St. Stephen’s Day'),\n        ('10-23', 'Republic Day'),\n        ('12-24', 'Christmas Eve'),\n        ('12-25', 'Christmas Day'),\n        ('12-26', 'Boxing Day')\n    ]\n}\n\nfrom datetime import date, timedelta\nimport dateutil.easter\n\nyears = list(range(2016, 2025))\n\n# Initialize dictionary for dynamically calculated holidays\ndynamic_holidays = {year: [] for year in years}\n\n# Compute dynamic holidays such as Easter and Mother's Day\nfor year in years:\n    # 📌 Paskalya (Easter) tatilleri\n    easter_sunday = dateutil.easter.easter(year)  # Easter Sunday\n    good_friday = easter_sunday - timedelta(days=2)  # Good Friday\n    easter_monday = easter_sunday + timedelta(days=1)  # Easter Monday\n    holy_saturday = easter_sunday - timedelta(days=1)  # Holy Saturday\n\n    # 📌 Anneler Günü (Mayıs'ın 2. Pazar günü)\n    may_first = date(year, 5, 1)\n    first_sunday_may = may_first + timedelta(days=(6 - may_first.weekday() + 7) % 7)\n    mother_day = first_sunday_may + timedelta(days=7)  # 2. Pazar\n\n    # 📌 Tatilleri ekleyelim\n    dynamic_holidays[year].extend([\n        (good_friday.strftime('%Y-%m-%d'), 'Good Friday'),\n        (easter_sunday.strftime('%Y-%m-%d'), 'Easter Day'),\n        (easter_monday.strftime('%Y-%m-%d'), 'Easter Monday'),\n        (holy_saturday.strftime('%Y-%m-%d'), 'Holy Saturday'),\n        (mother_day.strftime('%Y-%m-%d'), 'Mother Day')\n    ])\n\n\n# Map warehouses to corresponding countries\nwarehouse_country_map = {\n    'Frankfurt_1': 'Germany',\n    'Munich_1': 'Germany',\n    'Prague_1': 'Czechia',\n    'Prague_2': 'Czechia',\n    'Prague_3': 'Czechia',\n    'Brno_1': 'Czechia',\n    'Budapest_1': 'Hungary'\n}\n\n\nall_holidays = []\n\n# 📌 Her yıl için sabit tatilleri ekle\nfor year in years:\n    for country, holidays in fixed_holidays.items():\n        for date_suffix, holiday_name in holidays:\n            date_str = f\"{year}-{date_suffix}\"  # Örn: 2024-01-01\n            for warehouse in [w for w, c in warehouse_country_map.items() if c == country]:\n                all_holidays.append({\n                    'warehouse': warehouse,\n                    'date': date_str,\n                    'holiday_name': holiday_name,\n                    'holiday': 1,\n                    'shops_closed': 1,\n                    'winter_school_holidays': 1 if 'Christmas' in holiday_name else 0,\n                    'school_holidays': 1 if holiday_name in ['Labour Day', 'St. Wenceslas Day'] else 0\n                })\n\n# 📌 Her yıl için hareketli tatilleri ekle\nfor year, holidays in dynamic_holidays.items():\n    for date_str, holiday_name in holidays:\n        for country in fixed_holidays.keys():\n            for warehouse in [w for w, c in warehouse_country_map.items() if c == country]:\n                all_holidays.append({\n                    'warehouse': warehouse,\n                    'date': date_str,\n                    'holiday_name': holiday_name,\n                    'holiday': 1,\n                    'shops_closed': 1,\n                    'winter_school_holidays': 1 if 'Christmas' in holiday_name else 0,\n                    'school_holidays': 1 if holiday_name in ['Labour Day', 'St. Wenceslas Day'] else 0\n                })\n\n# Create a DataFrame from the holiday list\nholidays_df = pd.DataFrame(all_holidays)\nprint(holidays_df['holiday_name'].unique())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T21:34:28.740281Z","iopub.execute_input":"2025-02-10T21:34:28.740567Z","iopub.status.idle":"2025-02-10T21:34:28.758610Z","shell.execute_reply.started":"2025-02-10T21:34:28.740543Z","shell.execute_reply":"2025-02-10T21:34:28.757654Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to update calendar data with missing holidays\ndef update_calendar_holidays(calendar_df, holidays_df):\n    \"\"\"\n    Update existing calendar with new holiday information, only when there's actually a holiday\n    \n    Parameters:\n    calendar_df: Existing calendar DataFrame\n    holidays_df: New holidays DataFrame with updated information\n    \n    Returns:\n    Updated calendar DataFrame\n    \"\"\"\n    # Tarihleri datetime formatına çevir\n    calendar_df['date'] = pd.to_datetime(calendar_df['date'])\n    holidays_df['date'] = pd.to_datetime(holidays_df['date'])\n    \n    # Sadece gerçekten holiday olan günleri al\n    valid_holidays = holidays_df[holidays_df['holiday'] == 1].copy()\n    \n    # Güncellenmiş calendar'ı oluştur\n    calendar_updated = calendar_df.copy()\n    \n    # Güncellenecek kolonlar\n    update_columns = ['holiday_name', 'holiday', 'shops_closed', \n                     'winter_school_holidays', 'school_holidays']\n    \n    # Her bir holiday için güncelleme yap\n    for _, holiday_row in valid_holidays.iterrows():\n        mask = ((calendar_updated['warehouse'] == holiday_row['warehouse']) & \n                (calendar_updated['date'] == holiday_row['date']))\n        \n        if mask.any():\n            for col in update_columns:\n                calendar_updated.loc[mask, col] = holiday_row[col]\n    \n    # Tarihleri string formatına geri çevir\n    calendar_updated['date'] = calendar_updated['date'].dt.strftime('%Y-%m-%d')\n    \n    return calendar_updated\n\n# Güncellenmiş calendar'ı oluştur\ncalendar_updated = update_calendar_holidays(calendar, holidays_df)\n\n# Yapılan değişiklikleri kontrol et\nchanges = calendar_updated[\n    (calendar_updated['holiday_name'] != calendar['holiday_name']) & \n    (calendar_updated['holiday_name'].notna())  # Sadece geçerli holiday name'leri göster\n]\n\ncalendar = update_calendar_holidays(calendar, holidays_df)\n\nprint(f\"Toplam {len(changes)} satır güncellendi.\")\nprint(\"\\nÖrnek güncellemeler:\")\nprint(changes[['warehouse', 'date', 'holiday_name', 'holiday']].head())\n\n# Detaylı kontrol için\nprint(\"\\nGüncellenen tatil günleri dağılımı:\")\nprint(changes['holiday_name'].value_counts().head())\n\n\nprint(calendar['holiday_name'].unique())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T21:34:28.760310Z","iopub.execute_input":"2025-02-10T21:34:28.760675Z","iopub.status.idle":"2025-02-10T21:34:35.042754Z","shell.execute_reply.started":"2025-02-10T21:34:28.760636Z","shell.execute_reply":"2025-02-10T21:34:35.041893Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# --------------------------------------------------------\n# 📌 Holiday Mapping for Standardized Categorization\n# --------------------------------------------------------\n\n# Define a mapping for different holiday names into broader categories\nholiday_mapping = {\n    # 🎄 Christmas & Related Holidays\n    \"Christmas Holiday\": \"Christmas\",\n    \"Christmas Eve\": \"Christmas Eve\",\n    \"1st Christmas Day\": \"Christmas\",\n    \"2nd Christmas Day\": \"Christmas\",\n    \"Christmas Day\": \"Christmas\",\n    \"Boxing Day\": \"Boxing Day\",\n\n    # ✝️ Easter & Related Holidays\n    \"Easter Day\": \"Easter\",\n    \"Easter Monday\": \"Easter\",\n    \"Good Friday\": \"Easter\",\n    \"Holy Saturday\": \"Easter\",\n\n    # 🔥 Whitsun (Pentecost)\n    \"Whit sunday\": \"Whitsun\",\n    \"Whit monday\": \"Whitsun\",\n\n    # 🕊️ All Saints' Day\n    \"All Saints Day\": \"All Saints' Day\",\n    \"All Saints' Day Holiday\": \"All Saints' Day\",\n\n    # 🎆 New Year\n    \"New Years Day\": \"New Year\",\n    \"New Year\": \"New Year\",\n\n    # 🛠️ Labour Day\n    \"Labour Day\": \"Labor Day\",\n\n    # 🇨🇿 Czech Republic Holidays\n    \"Cyrila a Metodej\": \"Saints Cyril and Methodius Day\",\n    \"Jan Hus\": \"Jan Hus Day\",\n    \"St. Wenceslas Day\": \"Czech Statehood Day\",\n    \"Den vzniku samostatneho ceskoslovenskeho statu\": \"Czechoslovakia Independence Day\",\n    \"Czech Independence Day\": \"Czechoslovakia Independence Day\",\n    \"Den boje za svobodu a demokracii\": \"Freedom and Democracy Day\",\n    \"Den osvobozeni\": \"Victory Day\",\n    \"Den ceske statnosti\": \"Czech Statehood Day\",  # Eklenen mapping\n\n    # 🇭🇺 Hungary Holidays\n    \"1848 Revolution Memorial Day (Extra holiday)\": \"Hungary Revolution Memorial Day\",\n    \"Hungary National Day Holiday\": \"Hungary National Day\",\n    \"State Foundation Day\": \"Hungary National Day\",\n    \"Memorial day of the 1956 Revolution\": \"1956 Revolution Memorial Day\",\n    \"Independent Hungary Day\": \"Hungarian Independence Day\",\n    \"Memorial Day for the Victims of the Communist Dictatorships\": \"Communist Victims Memorial Day\",\n    \n    # 🇩🇪 Germany Holidays\n    \"German Unity Day\": \"German Unity Day\",\n\n    # 🌍 Other Holidays\n    \"International womens day\": \"International Women's Day\",\n    \"Ascension day\": \"Ascension Day\",\n    \"Corpus Christi\": \"Corpus Christi\",\n    \"Reformation Day\": \"Reformation Day\",\n    \"Assumption of the Virgin Mary\": \"Assumption of Mary\",\n    \"Epiphany\": \"Epiphany\",\n    \"Mother Day\": \"Mother's Day\",\n    \"Memorial Day of the Republic\": \"Republic Memorial Day\",\n    \"Memorial Day for the Victims of the Holocaust\": \"Holocaust Memorial Day\",\n    \"Memorial Day for the Martyrs of Arad\": \"Martyrs of Arad Memorial Day\",\n    \"Day of National Unity\": \"National Unity Day\",\n    \"National Defense Day\": \"National Defense Day\",\n    \"Peace Festival in Augsburg\": \"Peace Festival\",\n    \n    # Ek olarak; \n    \"weekend\": \"weekend\"  # \"weekend\" değeri de tek bir kategori altında toplanacak.\n}\n\n\n# 📌 Tatilleri haritalama işlemi\ncalendar[\"holiday_name_mapped\"] = calendar[\"holiday_name\"].map(holiday_mapping).fillna(calendar[\"holiday_name\"])\n\n# 📌 Mapping sonrası benzersiz tatilleri kontrol et\nprint(calendar[\"holiday_name_mapped\"].unique())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T21:34:35.043717Z","iopub.execute_input":"2025-02-10T21:34:35.044180Z","iopub.status.idle":"2025-02-10T21:34:35.057572Z","shell.execute_reply.started":"2025-02-10T21:34:35.044142Z","shell.execute_reply":"2025-02-10T21:34:35.056555Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# --------------------------------------------------------\n# 📌 Handling Weekends as Holidays\n# --------------------------------------------------------\ncalendar[\"date\"] = pd.to_datetime(calendar[\"date\"])\nmask = (calendar[\"date\"].dt.weekday >= 5) & calendar[\"holiday_name_mapped\"].isna()\ncalendar.loc[mask, \"holiday_name_mapped\"] = \"weekend\"\nprint(calendar[\"holiday_name_mapped\"].unique())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T21:34:35.058548Z","iopub.execute_input":"2025-02-10T21:34:35.058851Z","iopub.status.idle":"2025-02-10T21:34:35.087698Z","shell.execute_reply.started":"2025-02-10T21:34:35.058826Z","shell.execute_reply":"2025-02-10T21:34:35.086618Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"calendar['any_school_holiday'] = np.where(\n    (calendar['winter_school_holidays'] + calendar['school_holidays']) > 0,\n    1,\n    0\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T21:34:35.088719Z","iopub.execute_input":"2025-02-10T21:34:35.089062Z","iopub.status.idle":"2025-02-10T21:34:35.095413Z","shell.execute_reply.started":"2025-02-10T21:34:35.089033Z","shell.execute_reply":"2025-02-10T21:34:35.094425Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# --------------------------------------------------------\n# 📌 Adding Country Information for Warehouses\n# --------------------------------------------------------\ncalendar['country'] = calendar['warehouse'].map(warehouse_country_map)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"calendar=calendar.drop(columns=['holiday_name', 'winter_school_holidays', 'school_holidays'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T21:34:35.124454Z","iopub.execute_input":"2025-02-10T21:34:35.124740Z","iopub.status.idle":"2025-02-10T21:34:35.130714Z","shell.execute_reply.started":"2025-02-10T21:34:35.124717Z","shell.execute_reply":"2025-02-10T21:34:35.129909Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --------------------------------------------------------\n# 📌 Creating Shift Features for Holidays and Closures\n# --------------------------------------------------------\ncalendar=calendar.sort_values(['date']).reset_index(drop=True)\nfor gap in [-1,1,2,3,4,5,6,7]:\n        for col in ['any_school_holiday', 'holiday' , 'shops_closed']:\n            calendar[col+f\"_shift{gap}\"]=calendar.groupby(['warehouse'])[col].shift(gap)\n\n\n# Örnek: calendar DataFrame'inde, aşağıdaki önekler için işlemi yapalım:\nprefixes = ['any_school_holiday', 'holiday', 'shops_closed']\n\nfor prefix in prefixes:\n    # shift3'ten shift7'ye kadar olan sütun isimlerini oluşturuyoruz\n    cols = [f\"{prefix}_shift{i}\" for i in range(3, 8)]\n    # Sütunların hepsinin mevcut olduğunu kontrol edelim\n    existing_cols = [col for col in cols if col in calendar.columns]\n    if existing_cols:\n        # shift3 sütunu varsa; yoksa oluşturabiliriz\n        shift3_col = f\"{prefix}_shift3\"\n        if shift3_col in calendar.columns:\n            # Mevcut shift3 değerine diğer shift sütunlarının toplamını ekleyelim\n            calendar[shift3_col] = calendar[existing_cols].sum(axis=1)\n        else:\n            # Eğer shift3 sütunu yoksa, direkt toplamı atayalım\n            calendar[shift3_col] = calendar[existing_cols].sum(axis=1)\n        # shift4'ten shift7'ye kadar olan sütunları sil\n        cols_to_drop = [f\"{prefix}_shift{i}\" for i in range(4, 8) if f\"{prefix}_shift{i}\" in calendar.columns]\n        calendar.drop(columns=cols_to_drop, inplace=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T21:34:35.132653Z","iopub.execute_input":"2025-02-10T21:34:35.132990Z","iopub.status.idle":"2025-02-10T21:34:35.212339Z","shell.execute_reply.started":"2025-02-10T21:34:35.132954Z","shell.execute_reply":"2025-02-10T21:34:35.211205Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --------------------------------------------------------\n# 📌 Merge Calendar, Inventory, and Weights into the Main Dataset\n# --------------------------------------------------------\n\ndef data_frame_merge(df):\n    dt=df.merge(inventory, on=['unique_id'], how='left')\n    dt=dt.rename(columns={'warehouse_x': 'warehouse'})\n    dt[\"date\"] = pd.to_datetime(dt[\"date\"])\n    dt=dt.merge(calendar, on=['warehouse', 'date'], how='left')\n    dt=dt.merge(test_weights, on=['unique_id'], how='left')\n    \n\n    return dt\n\n\n\ntrain_merged  = data_frame_merge(combined_df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T21:34:35.213534Z","iopub.execute_input":"2025-02-10T21:34:35.214068Z","iopub.status.idle":"2025-02-10T21:34:44.777207Z","shell.execute_reply.started":"2025-02-10T21:34:35.214032Z","shell.execute_reply":"2025-02-10T21:34:44.776131Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Free memory by deleting unused variables\ndel calendar\ndel inventory\ndel sales_train\ndel sales_test\ndel solution\ndel test_weights\ndel combined_df\ndel holidays_df\ndel calendar_updated\ndel changes\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T21:34:44.786480Z","iopub.execute_input":"2025-02-10T21:34:44.786805Z","iopub.status.idle":"2025-02-10T21:34:44.857127Z","shell.execute_reply.started":"2025-02-10T21:34:44.786765Z","shell.execute_reply":"2025-02-10T21:34:44.856018Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Null Handling**","metadata":{}},{"cell_type":"code","source":"null_count = train_merged.isnull().sum()\nprint(null_count)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T21:34:44.858211Z","iopub.execute_input":"2025-02-10T21:34:44.858498Z","iopub.status.idle":"2025-02-10T21:34:46.881984Z","shell.execute_reply.started":"2025-02-10T21:34:44.858474Z","shell.execute_reply":"2025-02-10T21:34:46.880953Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.impute import KNNImputer\nfrom sklearn.preprocessing import StandardScaler\n\ndef fill_complex_nulls_by_group_iterative(df, \n                                          group_cols=['warehouse', 'unique_id'], \n                                          date_col='date', \n                                          fill_cols=None,\n                                          dist_cols=None,\n                                          n_neighbors=5, \n                                          weights=\"uniform\"):\n    \"\"\"\n    Bu fonksiyon, df içindeki fill_cols listesinde belirtilen sayısal kolonlardaki (NaN) eksikleri,\n    önce belirtilen grup sütunlarına göre (örn. warehouse, unique_id) gruplandırıp, \n    her grup içinde date kolonuna göre sıraladıktan sonra, kolon kolon (iteratif) doldurur.\n    \n    Her doldurma iterasyonunda, imputation için kullanılacak distance sütunları,\n    dist_cols listesinde belirtilir. Eğer doldurulması istenen kolon (örneğin 'sales'),\n    aynı anda distance sütunları arasında yer alıyorsa ve bu sütunlardan herhangi birinde eksik değer varsa,\n    o sütun bu iterasyonda predictor setine dahil edilmez.\n    \n    Parametreler:\n      - df: İşlenecek DataFrame.\n      - group_cols: Gruplama yapılacak sütunlar (varsayılan ['warehouse', 'unique_id']).\n      - date_col: Tarih bilgisini içeren sütun adı (varsayılan 'date').\n      - fill_cols: Doldurulması gereken kolonların listesi. Eğer None verilirse,\n                   örneğin ['total_orders', 'sales', 'sell_price_main', 'availability'] kullanılır.\n      - dist_cols: Mesafe hesaplamasında kullanılacak sütunların listesi. Örneğin: \n                   ['date', 'sell_price_main', 'total_orders', 'holiday'].\n                   Burada 'date' yerine, imputation sırasında 'date_numeric' kullanılacaktır.\n      - n_neighbors: KNN'de kullanılacak komşu sayısı.\n      - weights: Komşu ağırlıklandırma yöntemi (\"uniform\" veya \"distance\").\n    \"\"\"\n    if fill_cols is None:\n        fill_cols = ['availability', 'total_orders','sales' ]\n    if dist_cols is None:\n        dist_cols = ['date', 'sell_price_main', 'total_orders',  'availability','sales']\n    \n    # Öncelikle DataFrame'i grup sütunları ve tarih sütununa göre sıralıyoruz.\n    df = df.sort_values(group_cols + [date_col]).copy()\n    \n    def impute_group(group):\n        group = group.copy()\n        # Grup içindeki 'date' sütununu, en erken tarihten itibaren geçen gün sayısına çevirip 'date_numeric' olarak ekleyelim.\n        group['date_numeric'] = (group[date_col] - group[date_col].min()).dt.days\n        \n        # Her fill kolonu için iterasyon (sırasıyla doldurma)\n        for col in fill_cols:\n            missing_idx = group[group[col].isnull()].index\n            if len(missing_idx) == 0:\n                continue  # Bu kolonda eksik yoksa geç\n            \n            # İlk olarak, doldurulması istenen kolonu distance sütunları arasından çıkartıyoruz.\n            predictors = [p for p in dist_cols if p != col]\n            # 'date' sütunu varsa, onun yerine 'date_numeric' kullanılacak.\n            predictors = ['date_numeric' if p == 'date' else p for p in predictors]\n            \n            # Şimdi, eğer ilgili grup içinde herhangi bir predictor sütununda (predictors listesinde)\n            # eksik değer varsa, o sütunu predictor setinden çıkarıyoruz.\n            predictors = [p for p in predictors if not group[p].isnull().any()]\n            \n            # İmputation için kullanılacak sütun setini belirleyelim: doldurulması istenen kolon + predictors.\n            impute_features = [col] + predictors\n            # Eğer bazı impute_features grupta yoksa, onları çıkaralım.\n            impute_features = [feat for feat in impute_features if feat in group.columns]\n            \n            # Alt DataFrame oluşturup, sadece bu sütunları seçelim.\n            data = group[impute_features]\n            \n            # Ölçekleme: Tüm sütunları aynı ölçeğe getiriyoruz.\n            scaler = StandardScaler()\n            data_scaled = scaler.fit_transform(data)\n            \n            # KNN imputer ile eksik değerleri dolduralım.\n            imputer = KNNImputer(n_neighbors=n_neighbors, weights=weights)\n            data_imputed_scaled = imputer.fit_transform(data_scaled)\n            data_imputed = scaler.inverse_transform(data_imputed_scaled)\n            \n            # İmputed veriyi DataFrame olarak elde edelim.\n            data_imputed_df = pd.DataFrame(data_imputed, columns=impute_features, index=group.index)\n            \n            # Sadece doldurulması istenen kolon için imputed değerleri, eksik olan yerlerde güncelleyelim.\n            group.loc[missing_idx, col] = data_imputed_df.loc[missing_idx, col]\n        \n        # İşlem sonunda geçici eklenen 'date_numeric' sütununu kaldırıyoruz.\n        group.drop(columns=['date_numeric'], inplace=True)\n        return group\n    \n    # Gruplama işlemi: Her grup için imputation uyguluyoruz.\n    df_imputed = df.groupby(group_cols, group_keys=False).apply(impute_group)\n    return df_imputed\n\n# Örnek kullanım:\n# Varsayalım ki train_merged DataFrame'inizde 'warehouse', 'unique_id', 'date' gibi sütunlar ve doldurulması istenen\n# sayısal kolonlar (örneğin 'total_orders', 'sales', 'sell_price_main', 'availability') mevcut.\nfill_columns = ['availability', 'total_orders','sales' ]\ngroup_columns = ['warehouse', 'unique_id']\ndate_column = 'date'\n# Distance hesaplamasında kullanılacak sütunlar:\ndistance_columns = ['date', 'sell_price_main', 'total_orders', 'availability','sales']\n\n# Fonksiyonu çağırıyoruz:\ntrain_merged = fill_complex_nulls_by_group_iterative(train_merged, \n                                                     group_cols=group_columns, \n                                                     date_col=date_column, \n                                                     fill_cols=fill_columns, \n                                                     dist_cols=distance_columns, \n                                                     n_neighbors=5, \n                                                     weights=\"uniform\")\n\n# Sonuç: Her grup içinde, sıralamaya göre (tarih) ve belirlenen distance sütunlarına göre\n# eksik değerler, iteratif olarak (kolon bazında) doldurulmuş olacaktır.\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T21:34:46.883000Z","iopub.execute_input":"2025-02-10T21:34:46.883256Z","iopub.status.idle":"2025-02-10T21:36:05.898709Z","shell.execute_reply.started":"2025-02-10T21:34:46.883233Z","shell.execute_reply":"2025-02-10T21:36:05.897518Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Outlier Handling**","metadata":{}},{"cell_type":"code","source":"# I want to see outliers here\n\"\"\"\nfrom sklearn.ensemble import IsolationForest\n\n# Sadece sayısal sütunlarla çalış\nnumeric_cols = ['sales', 'total_orders', 'sell_price_main', 'availability']  # Örnek\nX = train_merged[numeric_cols].dropna()\n\n# Isolation Forest modeli\niso_forest = IsolationForest(n_estimators=100, contamination=0.02, random_state=42)\npreds = iso_forest.fit_predict(X)\n\n# Aykırı değerler (-1)\naykiri_veriler = train_merged[preds == -1]\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T21:36:05.900224Z","iopub.execute_input":"2025-02-10T21:36:05.900656Z","iopub.status.idle":"2025-02-10T21:36:05.906930Z","shell.execute_reply.started":"2025-02-10T21:36:05.900629Z","shell.execute_reply":"2025-02-10T21:36:05.906027Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import IsolationForest\nfrom scipy.stats.mstats import winsorize\nfrom sklearn.preprocessing import StandardScaler\n\ndef detect_and_correct_anomalies(dataframe, columns_to_check):\n    \"\"\"\n    Advanced anomaly detection and correction pipeline\n    \n    Parameters:\n    - dataframe: Input DataFrame\n    - columns_to_check: Numeric columns for anomaly detection\n    \n    Returns:\n    - Processed DataFrame with anomaly corrections\n    \"\"\"\n    \n    # Create a copy of the DataFrame to prevent modifications\n    df_processed = dataframe.copy()\n    \n    # 1. Prepare data for anomaly detection\n    data_for_detection = df_processed[columns_to_check].copy()\n    \n    # Handle any remaining missing values\n    data_for_detection.fillna(data_for_detection.median(), inplace=True)\n    \n    # 2. Standardize data for better anomaly detection\n    scaler = StandardScaler()\n    scaled_data = scaler.fit_transform(data_for_detection)\n    \n    # 3. Anomaly Detection using Isolation Forest\n    iso_forest = IsolationForest(\n        contamination=0.0001,  # 2% of data considered anomalous\n        random_state=42,\n        max_samples='auto',\n        bootstrap=False\n    )\n    \n    # Detect anomalies\n    anomaly_labels = iso_forest.fit_predict(scaled_data)\n    \n    # 4. Add anomaly flag to DataFrame\n    df_processed['is_anomaly'] = anomaly_labels == -1\n    \n    # 5. Detailed Anomaly Analysis\n    def calculate_anomaly_statistics(df, columns):\n        anomaly_stats = {}\n        for col in columns:\n            anomaly_stats[col] = {\n                'total_anomalies': df[df['is_anomaly']][col].count(),\n                'anomaly_percentage': (df['is_anomaly'].sum() / len(df)) * 100,\n                'original_mean': df[col].mean(),\n                'anomaly_mean': df[df['is_anomaly']][col].mean(),\n                'original_std': df[col].std(),\n                'anomaly_std': df[df['is_anomaly']][col].std()\n            }\n        return anomaly_stats\n    \n    anomaly_statistics = calculate_anomaly_statistics(df_processed, columns_to_check)\n    \n    # 6. Winsorization for Anomaly Correction\n    def winsorize_columns(df, columns, limits=(0.05, 0.05)):\n        df_winsorized = df.copy()\n        for col in columns:\n            df_winsorized[col] = winsorize(df_winsorized[col], limits=limits)\n        return df_winsorized\n    \n    # Apply Winsorization\n    df_winsorized = winsorize_columns(df_processed, columns_to_check)\n    \n    # 7. Correction Strategy\n    # Option 1: Replace anomalies with winsorized values\n    for col in columns_to_check:\n        df_processed.loc[df_processed['is_anomaly'], col] = df_winsorized.loc[df_processed['is_anomaly'], col]\n    \n    # 8. Additional Validation\n    def print_anomaly_report(stats):\n        print(\"Anomaly Detection Report:\")\n        for col, stat in stats.items():\n            print(f\"\\nColumn: {col}\")\n            print(f\"Total Anomalies: {stat['total_anomalies']}\")\n            print(f\"Anomaly Percentage: {stat['anomaly_percentage']:.2f}%\")\n            print(f\"Original Mean: {stat['original_mean']:.2f}\")\n            print(f\"Anomaly Mean: {stat['anomaly_mean']:.2f}\")\n    \n    print_anomaly_report(anomaly_statistics)\n    \n    return df_processed\n\n# Example Usage\ncolumns_to_analyze = [\n    'sales'\n]\n\n#outlier handling does not applied since it didn't improve results\n# Apply anomaly detection and correction\ntrain_merged_corrected = train_merged\n\n\"\"\"\n# Validation\nprint(\"\\nOriginal Train DataFrame Shape:\", train_merged.shape)\nprint(\"Corrected Train DataFrame Shape:\", train_merged_corrected.shape)\nprint(\"\\nAnomaly Detection Summary (Train):\")\nprint(\"Total Anomalies Detected:\", train_merged_corrected['is_anomaly'].sum())\n\"\"\"\n\ndel train_merged\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T21:36:05.907962Z","iopub.execute_input":"2025-02-10T21:36:05.908351Z","iopub.status.idle":"2025-02-10T21:36:05.954662Z","shell.execute_reply.started":"2025-02-10T21:36:05.908306Z","shell.execute_reply":"2025-02-10T21:36:05.953639Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Feature Engineering**","metadata":{}},{"cell_type":"markdown","source":"**Date Features**","metadata":{}},{"cell_type":"code","source":"\n\n\n\n# 📌 Tarih formatını düzelt ve sırala\ntrain_merged_corrected['date'] = pd.to_datetime(train_merged_corrected['date'])\ntrain_merged_corrected = train_merged_corrected.sort_values('date')\n\n\n# Tarihi bileşenlerine ayırma\ntrain_merged_corrected['date'] = pd.to_datetime(train_merged_corrected['date'])\ntrain_merged_corrected['year'] = train_merged_corrected['date'].dt.year\ntrain_merged_corrected['month'] = train_merged_corrected['date'].dt.month\ntrain_merged_corrected['day'] = train_merged_corrected['date'].dt.day\ntrain_merged_corrected['day_of_week'] = train_merged_corrected['date'].dt.dayofweek\ntrain_merged_corrected['is_weekend'] = train_merged_corrected['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)\ntrain_merged_corrected['week_of_year'] = train_merged_corrected['date'].dt.isocalendar().week\ntrain_merged_corrected['day_of_year']=train_merged_corrected['date'].dt.dayofyear\ntrain_merged_corrected['quarter'] = train_merged_corrected['date'].dt.quarter\n\n\n# --------------------------------------------------------\n# 📌 Creating Cyclical Features Using Sine and Cosine Transformations\n# --------------------------------------------------------\ntrain_merged_corrected['month_sin'] = np.sin(2 * np.pi * train_merged_corrected['month'] / 12) \ntrain_merged_corrected['month_cos'] = np.cos(2 * np.pi * train_merged_corrected['month'] / 12)\ntrain_merged_corrected['day_sin'] = np.sin(2 * np.pi * train_merged_corrected['day'] / 31)  \ntrain_merged_corrected['day_cos'] = np.cos(2 * np.pi * train_merged_corrected['day'] / 31)\ntrain_merged_corrected['sin_week']=np.sin(2*np.pi*train_merged_corrected['week_of_year']/52)\ntrain_merged_corrected['cos_week']=np.cos(2*np.pi*train_merged_corrected['week_of_year']/52)\ntrain_merged_corrected['sin_day_of_year']=np.sin(2*np.pi*train_merged_corrected['day_of_year']/365)\ntrain_merged_corrected['cos_day_of_year']=np.cos(2*np.pi*train_merged_corrected['day_of_year']/365)\ntrain_merged_corrected['day_of_week_sin'] = np.sin(\n    2 * np.pi * train_merged_corrected['day_of_week'] / 7\n)\ntrain_merged_corrected['day_of_week_cos'] = np.cos(\n    2 * np.pi * train_merged_corrected['day_of_week'] / 7\n)\ntrain_merged_corrected['quarter_sin'] = np.sin(\n    2 * np.pi * (train_merged_corrected['quarter']) / 4\n)\ntrain_merged_corrected['quarter_cos'] = np.cos(\n    2 * np.pi * (train_merged_corrected['quarter']) / 4\n)\n\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_merged_corrected=train_merged_corrected.drop(columns='warehouse_y')\ntrain_merged_corrected.info()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Trend Features**","metadata":{}},{"cell_type":"code","source":"\n\n# --------------------------------------------------------\n# 📌 Time Series Trend Features (Lag & Rolling Mean Features)\n# --------------------------------------------------------\ntrain_merged_corrected = train_merged_corrected.sort_values(by=['unique_id', 'warehouse', 'date'])\ndef create_time_features(df, windows=[7,14,20,28,35,84,356], group_cols=['unique_id', 'warehouse'], target_col='sales'):\n    \"\"\"\n    Create lag and rolling mean features for specified windows\n    Parameters:\n    df: pandas DataFrame\n    windows: list of integers representing the window sizes\n    group_cols: list of columns to group by\n    target_col: column to create features from\n    \n    Returns:\n    pandas DataFrame with new features added\n    \"\"\"\n    result_df = df.copy()\n    result_df = result_df.sort_values(by=['unique_id', 'warehouse', 'date'])\n    # Create lag features\n    for window in windows:\n        result_df[f'{target_col}_lag_{window}'] = (\n            result_df.groupby(group_cols)[target_col]\n            .shift(window)\n        )\n    # Create rolling mean features\n    for window in windows:\n        result_df[f'{target_col}_rolling_mean_{window}'] = (\n            result_df.groupby(group_cols)[target_col]\n            .rolling(window=window, min_periods=1)\n            .mean()\n            .reset_index(level=list(range(len(group_cols))), drop=True)\n        )\n    return result_df\n# Uygula\nwindows = [7,14,20,28,35,84,356]\ntrain_merged_corrected = create_time_features(\n    df=train_merged_corrected,\n    windows=windows,\n    group_cols=['unique_id', 'warehouse'],\n    target_col='sales'\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Sales and Prices Features**","metadata":{}},{"cell_type":"code","source":"\n\n# 📌 max sales\ntrain_merged_corrected['max_sales'] = (\n    train_merged_corrected.groupby(['unique_id', 'warehouse'])['sales']\n    .transform('max')\n)\n\n\n\n\ndiscount_columns = ['type_0_discount', 'type_1_discount', 'type_2_discount', 'type_3_discount', \n                    'type_4_discount', 'type_5_discount', 'type_6_discount']\n\ntrain_merged_corrected['max_discount'] = train_merged_corrected[discount_columns].max(axis=1)\ntrain_merged_corrected['total_discount'] = train_merged_corrected[discount_columns].apply(\n    lambda x: x[x >= 0].sum(), axis=1\n)\n\n# **Price Change (Fiyat Değişimi)**\ntrain_merged_corrected['price_change'] = train_merged_corrected.groupby(['unique_id', 'warehouse'])['sell_price_main'].pct_change()\n\n\n\n# 📌 'unique_id' kolonundaki alt çizgiden önceki kısmı çıkar\ntrain_merged_corrected['base_unique_id'] = train_merged_corrected['product_unique_id'].astype(str).str.split('_').str[0]\n\n# 📌 Yeni 'base_unique_id' bazında ortalama fiyat hesapla\naverage_price_df = (\n    train_merged_corrected.groupby('base_unique_id')['sell_price_main']\n    .mean()\n    .reset_index()\n    .rename(columns={'sell_price_main': 'avg_sell_price_main'})\n)\n\n# 📌 Ana veri setine ortalama fiyatı ekleme (Join işlemi)\ntrain_merged_corrected = train_merged_corrected.merge(average_price_df, on='base_unique_id', how='left')\n\n\n\ntrain_merged_corrected['product_name'] = train_merged_corrected['name'].str.split('_').str[0]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**GDP**","metadata":{}},{"cell_type":"code","source":"#deactivated because external data is prohibited\n\"\"\"\nimport wbdata\nimport pandas as pd\nfrom datetime import datetime\n\n# 1. Warehouse sütunundaki şehirleri ülkelere eşleştirme\nwarehouse_to_country = {\n    \"Budapest_1\": \"Hungary\",\n    \"Prague_2\": \"Czechia\",\n    \"Brno_1\": \"Czechia\",\n    \"Prague_1\": \"Czechia\",\n    \"Prague_3\": \"Czechia\",\n    \"Munich_1\": \"Germany\",\n    \"Frankfurt_1\": \"Germany\"\n}\n# Ülke isimlerini ekleyelim\ntrain_merged_corrected[\"country\"] = train_merged_corrected[\"warehouse\"].map(warehouse_to_country)\n\n# 2. Dünya Bankası göstergesi: GSYİH (GDP)\nindicator = {\"NY.GDP.MKTP.CD\": \"GDP\"}\n\n# 3. GSYİH verilerini çekme (convert_date kaldırıldı)\ngdp_data = wbdata.get_dataframe(indicator)\n\n# 4. İndeksi sıfırlayıp 'country' ve 'date' sütunlarını elde ediyoruz.\ngdp_data.reset_index(inplace=True)  \n# gdp_data sütunları örneğin: 'country', 'date', 'GDP' şeklinde olacak.\n\n# 5. 'date' sütununu datetime formatına çevirip 'year' sütununu oluşturuyoruz.\ngdp_data['date'] = pd.to_datetime(gdp_data['date'])\ngdp_data['year'] = gdp_data['date'].dt.year\n\n# 6. Gdp verilerinde kullanılan 'year' sütununun tipini int64'e dönüştürelim.\ngdp_data['year'] = gdp_data['year'].astype('int64')\n\n# 7. Train veri setinde, her satır için bir önceki yılı belirten \"prev_year\" sütununu oluşturuyoruz.\ntrain_merged_corrected[\"prev_year\"] = train_merged_corrected[\"year\"] - 1\n\n# 8. Train veri setindeki 'year' ve 'prev_year' sütunlarının tipini int64 yapalım.\ntrain_merged_corrected[\"year\"] = train_merged_corrected[\"year\"].astype('int64')\ntrain_merged_corrected[\"prev_year\"] = train_merged_corrected[\"prev_year\"].astype('int64')\n\n# 9. GSYİH verilerini, ülke ve \"prev_year\" bilgisi üzerinden birleştiriyoruz.\n#    Merge işlemi sol tarafta \"prev_year\" ile, sağ tarafta \"year\" sütunu üzerinden gerçekleşecek.\ntrain_merged_corrected = train_merged_corrected.merge(\n    gdp_data,\n    how=\"left\",\n    left_on=[\"country\", \"prev_year\"],\n    right_on=[\"country\", \"year\"]\n)\n\n# 10. Merge sonrası gelen GDP sütununu \"GDP_prev\" olarak yeniden adlandırıyoruz.\ntrain_merged_corrected.rename(columns={\"year_x\": \"year\"}, inplace=True)\ntrain_merged_corrected.rename(columns={\"date_x\": \"date\"}, inplace=True)\n\ntrain_merged_corrected.drop(columns=[  \"year_y\"], errors=\"ignore\", inplace=True)\n\n# Sonuç kontrolü:\nprint(train_merged_corrected[['warehouse', 'year', 'prev_year', 'country', 'GDP']].head())\n\"\"\"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef create_statistical_features_optimized(df, target_col, group_cols=['unique_id', 'warehouse']):\n    \"\"\"\n    Verilen veri seti üzerinde, group_cols bazında target_col için çeşitli istatistiksel öznitelikler oluşturur.\n    Tek bir groupby-aggregation ile hesaplanan öznitelikler, orijinal df'e merge edilir.\n\n    Parameters:\n      df: pandas DataFrame\n      group_cols: gruplama için kullanılacak sütunlar\n      target_col: istatistiklerin hesaplanacağı hedef sütun\n\n    Returns:\n      Yeni özniteliklerle genişletilmiş pandas DataFrame\n    \"\"\"\n    # Orijinal veri setini kopyalıyoruz.\n    result_df = df.copy()\n\n    # 1) Tek seferde groupby-agg sözlüğü\n    agg_dict = {\n        f'{target_col}_max': 'max',\n        f'{target_col}_mean': 'mean',\n        f'{target_col}_median': 'median',\n        f'{target_col}_std': 'std',\n        f'{target_col}_skew': 'skew',\n        f'{target_col}_zero_ratio': lambda x: (x == 0).mean()\n    }\n\n    # 2) groupby ile özet tablomuza çeviriyoruz\n    agg_df = (\n        result_df\n        .groupby(group_cols)[target_col]\n        .agg(**agg_dict)\n        .reset_index()\n    )\n\n\n    # 4) Orijinal df ile hesaplanan özellikleri merge\n    result_df = result_df.merge(agg_df, on=group_cols, how='left')\n\n    return result_df\n\n# Örnek kullanım\nfor col in ['sales', 'total_orders', 'sell_price_main', 'total_discount']:\n    train_merged_corrected = create_statistical_features_optimized(\n        df=train_merged_corrected,\n        target_col=col,\n        group_cols=['unique_id', 'warehouse']\n    )\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef reduce_mem_usage(df):\n    start_mem = df.memory_usage().sum() / 1024**2\n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type not in [object, 'category', 'datetime64[ns]']:\n            c_min = df[col].min()\n            c_max = df[col].max()\n\n            # tamsayı sütunlar\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                else:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                # float sütunlar\n                df[col] = df[col].astype(np.float32)\n                \n        elif col_type == object:\n            # Eğer gerçekte kategorik veya sayısal değilse, kategorik dönüştürebilirsiniz\n            # df[col] = df[col].astype('category')\n            pass\n    \n    end_mem = df.memory_usage().sum() / 1024**2\n    print(f\"Bellek kullanımı: {start_mem:.2f} MB -> {end_mem:.2f} MB\")\n    return df\ntrain_merged_corrected=  reduce_mem_usage  (train_merged_corrected)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\ndef create_statistical_features_optimized2(df, target_col, group_cols=['unique_id', 'warehouse','year']):\n    \"\"\"\n    Verilen veri seti üzerinde, group_cols bazında target_col için çeşitli istatistiksel öznitelikler oluşturur.\n    Tek bir groupby-aggregation ile hesaplanan öznitelikler, orijinal df'e merge edilir.\n\n    Parameters:\n      df: pandas DataFrame\n      group_cols: gruplama için kullanılacak sütunlar\n      target_col: istatistiklerin hesaplanacağı hedef sütun\n\n    Returns:\n      Yeni özniteliklerle genişletilmiş pandas DataFrame\n    \"\"\"\n    # Orijinal veri setini kopyalıyoruz.\n    result_df = df.copy()\n\n    # 1) Tek seferde groupby-agg sözlüğü\n    agg_dict = {\n        f'{target_col}_yearly_max': 'max',\n        f'{target_col}_yearly_min': 'min',\n        f'{target_col}_yearly_mean': 'mean',\n        f'{target_col}_yearly_median': 'median',\n        f'{target_col}_yearly_std': 'std',\n        f'{target_col}_yearly_skew': 'skew',\n        f'{target_col}_yearly_q25': lambda x: x.quantile(0.25),\n        f'{target_col}_yearly_q75': lambda x: x.quantile(0.75),\n        f'{target_col}_yearly_zero_ratio': lambda x: (x == 0).mean()\n    }\n\n    # 2) groupby ile özet tablomuza çeviriyoruz\n    agg_df = (\n        result_df\n        .groupby(group_cols)[target_col]\n        .agg(**agg_dict)\n        .reset_index()\n    )\n\n    # 3) CV (Coefficient of Variation) = std / mean\n    #    mean=0 durumunda bölme hatası almamak için 0'ları NaN'a çeviriyoruz\n    agg_df[f'{target_col}_yearly_cv'] = (\n        agg_df[f'{target_col}_yearly_std'] / agg_df[f'{target_col}_yearly_mean'].replace(0, np.nan)\n    )\n\n    # 4) Orijinal df ile hesaplanan özellikleri merge\n    result_df = result_df.merge(agg_df, on=group_cols, how='left')\n\n    return result_df\n\n# Örnek kullanım\nfor col in ['sales', 'total_orders', 'sell_price_main', 'availability', 'total_discount']:\n    train_merged_corrected = create_statistical_features_optimized2(\n        df=train_merged_corrected,\n        target_col=col,\n        group_cols=['unique_id', 'warehouse','year']\n    )\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# Tarih sütununu datetime formatına çeviriyoruz.\ntrain_merged_corrected['date'] = pd.to_datetime(train_merged_corrected['date'])\ntrain_merged_corrected['holiday_name'] = train_merged_corrected['holiday_name_mapped'].fillna(\"nan\")\n# Geçersiz tatil değerleri\ninvalid_values = ['weekend', 'nan']\n\ndef assign_holiday_columns_vectorized(group):\n    # Her warehouse için tarih sıralaması\n    group = group.sort_values('date').reset_index(drop=True)\n    # Sadece geçerli tatilleri filtreleyelim\n    valid = group.loc[(~group['holiday_name'].isin(invalid_values)) & (group['holiday_name'].notna())].copy()\n    \n    # Eğer geçerli tatil yoksa, ilgili sütunları NaN ile dolduruyoruz.\n    if valid.empty:\n        group['next_5_days_holidays'] = np.nan\n        group['next_15_days_holidays'] = np.nan\n        group['prev_5_days_holidays'] = np.nan\n        return group\n    \n    # --- SONRAKİ TATİLLER İÇİN ---\n    # Arama için: mevcut tarihe 1 gün ekleyelim.\n    group['search_date'] = group['date'] + pd.Timedelta(days=1)\n    # merge_asof ile, search_date'e en yakın (ilerideki) geçerli tatili buluyoruz.\n    merged_next = pd.merge_asof(\n        group.sort_values('search_date'),\n        valid.sort_values('date'),\n        left_on='search_date',\n        right_on='date',\n        direction='forward',\n        suffixes=('', '_hol')\n    )\n    merged_next = merged_next.sort_index()\n    \n    # Vektörize koşullu atamalar:\n\n    group['next_5_days_holidays'] = np.where(\n        merged_next['date_hol'] <= group['date'] + pd.Timedelta(days=5),\n        merged_next['holiday_name_hol'], np.nan\n    )\n    group['next_15_days_holidays'] = np.where(\n        merged_next['date_hol'] <= group['date'] + pd.Timedelta(days=15),\n        merged_next['holiday_name_hol'], np.nan\n    )\n    \n    # --- ÖNCEKİ TATİLLER İÇİN ---\n    # Arama için: mevcut tarihten 1 gün çıkaralım.\n    group['search_date_prev'] = group['date'] - pd.Timedelta(days=1)\n    # merge_asof ile, search_date_prev'e en yakın (gerideki) geçerli tatili buluyoruz.\n    merged_prev = pd.merge_asof(\n        group.sort_values('search_date_prev'),\n        valid.sort_values('date'),\n        left_on='search_date_prev',\n        right_on='date',\n        direction='backward',\n        suffixes=('', '_hol')\n    )\n    merged_prev = merged_prev.sort_index()\n    \n\n    group['prev_5_days_holidays'] = np.where(\n        merged_prev['date_hol'] >= group['date'] - pd.Timedelta(days=5),\n        merged_prev['holiday_name_hol'], np.nan\n    )\n\n    \n    # Geçici sütunları temizliyoruz.\n    group.drop(columns=['search_date', 'search_date_prev'], inplace=True)\n    \n    return group\n\n# Her warehouse için gruplandırarak vektörize işlemi uyguluyoruz.\ntrain_merged_corrected = train_merged_corrected.groupby('warehouse', group_keys=False).apply(assign_holiday_columns_vectorized)\n\n# Sonuçları kontrol edelim.\nprint(train_merged_corrected.head())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Örnek pandemi dönemlerini tanımlayan fonksiyon:\ndef get_pandemic_phase(date):\n    \"\"\"\n    Bu fonksiyon, verilen date değeri için pandemi dönemini belirler.\n    \n    Örnek dönemler:\n      - \"pre-pandemic\": 11 Mart 2020 öncesi\n      - \"pandemic\": 11 Mart 2020 - 31 Aralık 2021\n      - \"post-pandemic\": 1 Ocak 2022 - 31 Mayıs 2022\n      - \"healthy\": 1 Haziran 2022 sonrası\n      \n    Bu dönemler, analiz ihtiyaçlarınıza göre yeniden düzenlenebilir.\n    \"\"\"\n    if date < pd.Timestamp(\"2020-03-11\"):\n        return \"pre-pandemic\"\n    elif date <= pd.Timestamp(\"2021-12-31\"):\n        return \"pandemic\"\n    elif date <= pd.Timestamp(\"2022-05-31\"):\n        return \"post-pandemic\"\n    else:\n        return \"healthy\"\n\n# Tarih sütununu zaten datetime formatına çevirdiğinizi varsayarsak:\ntrain_merged_corrected[\"pandemic_phase\"] = train_merged_corrected[\"date\"].apply(get_pandemic_phase)\n\n# Örnek olarak ilk 5 satırı görüntüleyelim:\nprint(train_merged_corrected[[\"date\", \"pandemic_phase\"]].head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Reset index to ensure uniqueness\ntrain_merged_corrected = train_merged_corrected.reset_index(drop=True)\ntrain_merged_corrected = train_merged_corrected.sort_values(['warehouse','date'])\n\n# Now, create the 'next_shops_closed_date' column:\ntrain_merged_corrected['next_shops_closed_date'] = train_merged_corrected.loc[train_merged_corrected['shops_closed'] == 1, 'date'].shift(-1)\ntrain_merged_corrected['next_shops_closed_date'] = train_merged_corrected['next_shops_closed_date'].bfill()\ntrain_merged_corrected['days_to_next_closed'] = (train_merged_corrected['next_shops_closed_date'] - train_merged_corrected['date']).dt.days\n\n# Create a binary column indicating if the day immediately after a closed day (shops_closed==1) is open:\ntrain_merged_corrected['day_after_closing'] = ((train_merged_corrected['shops_closed'] == 0) & (train_merged_corrected['shops_closed'].shift(1) == 1)).astype(int)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Her grup için flag hesaplama: 1500, 5200 ve 10000 eşiğine göre holiday koşullu ve koşulsuz flag'lar\nflag_df = train_merged_corrected.groupby(['warehouse', 'unique_id']).apply(\n    lambda df: pd.Series({\n        'high_sales_1500_holidaysiz': int((df['sales'] > 1500).any()),\n        'high_sales_5200_holidaysiz': int((df['sales'] > 5200).any()),\n        'high_sales_10000_holidaysiz': int((df['sales'] > 10000).any())\n    })\n).reset_index()\n\n# Orijinal veri setine flag'ları merge edelim\ntrain_merged_corrected = train_merged_corrected.merge(flag_df, on=['warehouse', 'unique_id'], how='left')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ncolumns_to_drop = [\n    'unique_id', 'date'\n]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"del average_price_df\ndel flag_df\ntrain_merged_corrected=train_merged_corrected.drop(columns=['holiday_name',\n 'next_shops_closed_date', 'base_unique_id','day_after_closing', 'max_sales'])\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T05:54:15.562474Z","iopub.execute_input":"2025-02-11T05:54:15.562838Z","iopub.status.idle":"2025-02-11T05:54:15.647692Z","shell.execute_reply.started":"2025-02-11T05:54:15.562796Z","shell.execute_reply":"2025-02-11T05:54:15.646264Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\ndef reduce_mem_usage(df):\n    start_mem = df.memory_usage().sum() / 1024**2\n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type not in [object, 'category', 'datetime64[ns]']:\n            c_min = df[col].min()\n            c_max = df[col].max()\n\n            # tamsayı sütunlar\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                else:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                # float sütunlar\n                df[col] = df[col].astype(np.float32)\n                \n        elif col_type == object:\n            # Eğer gerçekte kategorik veya sayısal değilse, kategorik dönüştürebilirsiniz\n            # df[col] = df[col].astype('category')\n            pass\n    \n    end_mem = df.memory_usage().sum() / 1024**2\n    print(f\"Bellek kullanımı: {start_mem:.2f} MB -> {end_mem:.2f} MB\")\n    return df\ntrain_merged_corrected=  reduce_mem_usage  (train_merged_corrected)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Export without index for easier reading later\ntrain_merged_corrected.to_csv(\"train_merged_corrected.csv\", index=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}