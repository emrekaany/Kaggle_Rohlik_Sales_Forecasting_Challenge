{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":88742,"databundleVersionId":10173359,"sourceType":"competition"}],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Data preprocessing and feature engineering is applied for the following notebooks.\n\nimport numpy as np  # Linear algebra operations\nimport pandas as pd  # Data processing, CSV file I/O (e.g., pd.read_csv)\nimport gc  # Garbage collection for memory management\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-11T06:10:07.087904Z","iopub.execute_input":"2025-02-11T06:10:07.088302Z","iopub.status.idle":"2025-02-11T06:10:07.520204Z","shell.execute_reply.started":"2025-02-11T06:10:07.088268Z","shell.execute_reply":"2025-02-11T06:10:07.518827Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Load datasets\ncalendar = pd.read_csv('/kaggle/input/rohlik-sales-forecasting-challenge-v2/calendar.csv')\ninventory = pd.read_csv('/kaggle/input/rohlik-sales-forecasting-challenge-v2/inventory.csv')\nsales_train = pd.read_csv('/kaggle/input/rohlik-sales-forecasting-challenge-v2/sales_train.csv')\nsales_test = pd.read_csv('/kaggle/input/rohlik-sales-forecasting-challenge-v2/sales_test.csv')\nsolution = pd.read_csv('/kaggle/input/rohlik-sales-forecasting-challenge-v2/solution.csv')\ntest_weights= pd.read_csv('/kaggle/input/rohlik-sales-forecasting-challenge-v2/test_weights.csv')\n\n\n# Add an empty (NaN) target column to test set since it is missing\nsales_test['sales'] = np.nan\n\n# Combine train and test datasets to ensure consistent processing\ncombined_df = pd.concat([sales_train, sales_test], ignore_index=True)\n\n# Convert date columns to datetime format for better handling\nsales_train['date'] = pd.to_datetime(sales_train['date'])\nsales_test['date'] = pd.to_datetime(sales_test['date'])\n\n# Compute the minimum and maximum dates for train and test sets\ntrain_min_date = sales_train['date'].min()\ntrain_max_date = sales_train['date'].max()\ntest_min_date = sales_test['date'].min()\ntest_max_date = sales_test['date'].max()\n\n# SonuÃ§larÄ± yazdÄ±rma\nprint(\"Sales Train - En KÃ¼Ã§Ã¼k Tarih:\", train_min_date)\nprint(\"Sales Train - En BÃ¼yÃ¼k Tarih:\", train_max_date)\nprint(\"Sales Test - En KÃ¼Ã§Ã¼k Tarih:\", test_min_date)\nprint(\"Sales Test - En BÃ¼yÃ¼k Tarih:\", test_max_date)\n   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T06:10:07.521812Z","iopub.execute_input":"2025-02-11T06:10:07.522505Z","iopub.status.idle":"2025-02-11T06:10:16.661990Z","shell.execute_reply.started":"2025-02-11T06:10:07.522423Z","shell.execute_reply":"2025-02-11T06:10:16.660953Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **DATA QUALITY**","metadata":{}},{"cell_type":"markdown","source":"**Missing Holiday Handling**","metadata":{}},{"cell_type":"code","source":"#Take all holidays\nfixed_holidays = {\n    'Germany': [\n        ('01-01', 'New Year'),\n        ('05-01', 'Labour Day'),\n        ('10-03', 'German Unity Day'),\n        ('12-24', 'Christmas Eve'),\n        ('12-25', 'Christmas Day'),\n        ('12-26', 'Boxing Day')\n    ],\n    'Czechia': [\n        ('01-01', 'New Year'),\n        ('05-01', 'Labour Day'),\n        ('09-28', 'St. Wenceslas Day'),\n        ('10-28', 'Czech Independence Day'),\n        ('12-24', 'Christmas Eve'),\n        ('12-25', 'Christmas Day'),\n        ('12-26', 'Boxing Day')\n    ],\n    'Hungary': [\n        ('01-01', 'New Year'),\n        ('03-15', 'Revolution Day'),\n        ('05-01', 'Labour Day'),\n        ('08-20', 'St. Stephenâ€™s Day'),\n        ('10-23', 'Republic Day'),\n        ('12-24', 'Christmas Eve'),\n        ('12-25', 'Christmas Day'),\n        ('12-26', 'Boxing Day')\n    ]\n}\n\nfrom datetime import date, timedelta\nimport dateutil.easter\n\nyears = list(range(2016, 2025))\n\n# Initialize dictionary for dynamically calculated holidays\ndynamic_holidays = {year: [] for year in years}\n\n# Compute dynamic holidays such as Easter and Mother's Day\nfor year in years:\n    # ğŸ“Œ Paskalya (Easter) tatilleri\n    easter_sunday = dateutil.easter.easter(year)  # Easter Sunday\n    good_friday = easter_sunday - timedelta(days=2)  # Good Friday\n    easter_monday = easter_sunday + timedelta(days=1)  # Easter Monday\n    holy_saturday = easter_sunday - timedelta(days=1)  # Holy Saturday\n\n    # ğŸ“Œ Anneler GÃ¼nÃ¼ (MayÄ±s'Ä±n 2. Pazar gÃ¼nÃ¼)\n    may_first = date(year, 5, 1)\n    first_sunday_may = may_first + timedelta(days=(6 - may_first.weekday() + 7) % 7)\n    mother_day = first_sunday_may + timedelta(days=7)  # 2. Pazar\n\n    # ğŸ“Œ Tatilleri ekleyelim\n    dynamic_holidays[year].extend([\n        (good_friday.strftime('%Y-%m-%d'), 'Good Friday'),\n        (easter_sunday.strftime('%Y-%m-%d'), 'Easter Day'),\n        (easter_monday.strftime('%Y-%m-%d'), 'Easter Monday'),\n        (holy_saturday.strftime('%Y-%m-%d'), 'Holy Saturday'),\n        (mother_day.strftime('%Y-%m-%d'), 'Mother Day')\n    ])\n\n\n# Map warehouses to corresponding countries\nwarehouse_country_map = {\n    'Frankfurt_1': 'Germany',\n    'Munich_1': 'Germany',\n    'Prague_1': 'Czechia',\n    'Prague_2': 'Czechia',\n    'Prague_3': 'Czechia',\n    'Brno_1': 'Czechia',\n    'Budapest_1': 'Hungary'\n}\n\n\nall_holidays = []\n\n# ğŸ“Œ Her yÄ±l iÃ§in sabit tatilleri ekle\nfor year in years:\n    for country, holidays in fixed_holidays.items():\n        for date_suffix, holiday_name in holidays:\n            date_str = f\"{year}-{date_suffix}\"  # Ã–rn: 2024-01-01\n            for warehouse in [w for w, c in warehouse_country_map.items() if c == country]:\n                all_holidays.append({\n                    'warehouse': warehouse,\n                    'date': date_str,\n                    'holiday_name': holiday_name,\n                    'holiday': 1,\n                    'shops_closed': 1,\n                    'winter_school_holidays': 1 if 'Christmas' in holiday_name else 0,\n                    'school_holidays': 1 if holiday_name in ['Labour Day', 'St. Wenceslas Day'] else 0\n                })\n\n# ğŸ“Œ Her yÄ±l iÃ§in hareketli tatilleri ekle\nfor year, holidays in dynamic_holidays.items():\n    for date_str, holiday_name in holidays:\n        for country in fixed_holidays.keys():\n            for warehouse in [w for w, c in warehouse_country_map.items() if c == country]:\n                all_holidays.append({\n                    'warehouse': warehouse,\n                    'date': date_str,\n                    'holiday_name': holiday_name,\n                    'holiday': 1,\n                    'shops_closed': 1,\n                    'winter_school_holidays': 1 if 'Christmas' in holiday_name else 0,\n                    'school_holidays': 1 if holiday_name in ['Labour Day', 'St. Wenceslas Day'] else 0\n                })\n\n# Create a DataFrame from the holiday list\nholidays_df = pd.DataFrame(all_holidays)\nprint(holidays_df['holiday_name'].unique())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T21:34:28.740281Z","iopub.execute_input":"2025-02-10T21:34:28.740567Z","iopub.status.idle":"2025-02-10T21:34:28.758610Z","shell.execute_reply.started":"2025-02-10T21:34:28.740543Z","shell.execute_reply":"2025-02-10T21:34:28.757654Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to update calendar data with missing holidays\ndef update_calendar_holidays(calendar_df, holidays_df):\n    \"\"\"\n    Update existing calendar with new holiday information, only when there's actually a holiday\n    \n    Parameters:\n    calendar_df: Existing calendar DataFrame\n    holidays_df: New holidays DataFrame with updated information\n    \n    Returns:\n    Updated calendar DataFrame\n    \"\"\"\n    # Tarihleri datetime formatÄ±na Ã§evir\n    calendar_df['date'] = pd.to_datetime(calendar_df['date'])\n    holidays_df['date'] = pd.to_datetime(holidays_df['date'])\n    \n    # Sadece gerÃ§ekten holiday olan gÃ¼nleri al\n    valid_holidays = holidays_df[holidays_df['holiday'] == 1].copy()\n    \n    # GÃ¼ncellenmiÅŸ calendar'Ä± oluÅŸtur\n    calendar_updated = calendar_df.copy()\n    \n    # GÃ¼ncellenecek kolonlar\n    update_columns = ['holiday_name', 'holiday', 'shops_closed', \n                     'winter_school_holidays', 'school_holidays']\n    \n    # Her bir holiday iÃ§in gÃ¼ncelleme yap\n    for _, holiday_row in valid_holidays.iterrows():\n        mask = ((calendar_updated['warehouse'] == holiday_row['warehouse']) & \n                (calendar_updated['date'] == holiday_row['date']))\n        \n        if mask.any():\n            for col in update_columns:\n                calendar_updated.loc[mask, col] = holiday_row[col]\n    \n    # Tarihleri string formatÄ±na geri Ã§evir\n    calendar_updated['date'] = calendar_updated['date'].dt.strftime('%Y-%m-%d')\n    \n    return calendar_updated\n\n# GÃ¼ncellenmiÅŸ calendar'Ä± oluÅŸtur\ncalendar_updated = update_calendar_holidays(calendar, holidays_df)\n\n# YapÄ±lan deÄŸiÅŸiklikleri kontrol et\nchanges = calendar_updated[\n    (calendar_updated['holiday_name'] != calendar['holiday_name']) & \n    (calendar_updated['holiday_name'].notna())  # Sadece geÃ§erli holiday name'leri gÃ¶ster\n]\n\ncalendar = update_calendar_holidays(calendar, holidays_df)\n\nprint(f\"Toplam {len(changes)} satÄ±r gÃ¼ncellendi.\")\nprint(\"\\nÃ–rnek gÃ¼ncellemeler:\")\nprint(changes[['warehouse', 'date', 'holiday_name', 'holiday']].head())\n\n# DetaylÄ± kontrol iÃ§in\nprint(\"\\nGÃ¼ncellenen tatil gÃ¼nleri daÄŸÄ±lÄ±mÄ±:\")\nprint(changes['holiday_name'].value_counts().head())\n\n\nprint(calendar['holiday_name'].unique())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T21:34:28.760310Z","iopub.execute_input":"2025-02-10T21:34:28.760675Z","iopub.status.idle":"2025-02-10T21:34:35.042754Z","shell.execute_reply.started":"2025-02-10T21:34:28.760636Z","shell.execute_reply":"2025-02-10T21:34:35.041893Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# --------------------------------------------------------\n# ğŸ“Œ Holiday Mapping for Standardized Categorization\n# --------------------------------------------------------\n\n# Define a mapping for different holiday names into broader categories\nholiday_mapping = {\n    # ğŸ„ Christmas & Related Holidays\n    \"Christmas Holiday\": \"Christmas\",\n    \"Christmas Eve\": \"Christmas Eve\",\n    \"1st Christmas Day\": \"Christmas\",\n    \"2nd Christmas Day\": \"Christmas\",\n    \"Christmas Day\": \"Christmas\",\n    \"Boxing Day\": \"Boxing Day\",\n\n    # âœï¸ Easter & Related Holidays\n    \"Easter Day\": \"Easter\",\n    \"Easter Monday\": \"Easter\",\n    \"Good Friday\": \"Easter\",\n    \"Holy Saturday\": \"Easter\",\n\n    # ğŸ”¥ Whitsun (Pentecost)\n    \"Whit sunday\": \"Whitsun\",\n    \"Whit monday\": \"Whitsun\",\n\n    # ğŸ•Šï¸ All Saints' Day\n    \"All Saints Day\": \"All Saints' Day\",\n    \"All Saints' Day Holiday\": \"All Saints' Day\",\n\n    # ğŸ† New Year\n    \"New Years Day\": \"New Year\",\n    \"New Year\": \"New Year\",\n\n    # ğŸ› ï¸ Labour Day\n    \"Labour Day\": \"Labor Day\",\n\n    # ğŸ‡¨ğŸ‡¿ Czech Republic Holidays\n    \"Cyrila a Metodej\": \"Saints Cyril and Methodius Day\",\n    \"Jan Hus\": \"Jan Hus Day\",\n    \"St. Wenceslas Day\": \"Czech Statehood Day\",\n    \"Den vzniku samostatneho ceskoslovenskeho statu\": \"Czechoslovakia Independence Day\",\n    \"Czech Independence Day\": \"Czechoslovakia Independence Day\",\n    \"Den boje za svobodu a demokracii\": \"Freedom and Democracy Day\",\n    \"Den osvobozeni\": \"Victory Day\",\n    \"Den ceske statnosti\": \"Czech Statehood Day\",  # Eklenen mapping\n\n    # ğŸ‡­ğŸ‡º Hungary Holidays\n    \"1848 Revolution Memorial Day (Extra holiday)\": \"Hungary Revolution Memorial Day\",\n    \"Hungary National Day Holiday\": \"Hungary National Day\",\n    \"State Foundation Day\": \"Hungary National Day\",\n    \"Memorial day of the 1956 Revolution\": \"1956 Revolution Memorial Day\",\n    \"Independent Hungary Day\": \"Hungarian Independence Day\",\n    \"Memorial Day for the Victims of the Communist Dictatorships\": \"Communist Victims Memorial Day\",\n    \n    # ğŸ‡©ğŸ‡ª Germany Holidays\n    \"German Unity Day\": \"German Unity Day\",\n\n    # ğŸŒ Other Holidays\n    \"International womens day\": \"International Women's Day\",\n    \"Ascension day\": \"Ascension Day\",\n    \"Corpus Christi\": \"Corpus Christi\",\n    \"Reformation Day\": \"Reformation Day\",\n    \"Assumption of the Virgin Mary\": \"Assumption of Mary\",\n    \"Epiphany\": \"Epiphany\",\n    \"Mother Day\": \"Mother's Day\",\n    \"Memorial Day of the Republic\": \"Republic Memorial Day\",\n    \"Memorial Day for the Victims of the Holocaust\": \"Holocaust Memorial Day\",\n    \"Memorial Day for the Martyrs of Arad\": \"Martyrs of Arad Memorial Day\",\n    \"Day of National Unity\": \"National Unity Day\",\n    \"National Defense Day\": \"National Defense Day\",\n    \"Peace Festival in Augsburg\": \"Peace Festival\",\n    \n    # Ek olarak; \n    \"weekend\": \"weekend\"  # \"weekend\" deÄŸeri de tek bir kategori altÄ±nda toplanacak.\n}\n\n\n# ğŸ“Œ Tatilleri haritalama iÅŸlemi\ncalendar[\"holiday_name_mapped\"] = calendar[\"holiday_name\"].map(holiday_mapping).fillna(calendar[\"holiday_name\"])\n\n# ğŸ“Œ Mapping sonrasÄ± benzersiz tatilleri kontrol et\nprint(calendar[\"holiday_name_mapped\"].unique())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T21:34:35.043717Z","iopub.execute_input":"2025-02-10T21:34:35.044180Z","iopub.status.idle":"2025-02-10T21:34:35.057572Z","shell.execute_reply.started":"2025-02-10T21:34:35.044142Z","shell.execute_reply":"2025-02-10T21:34:35.056555Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# --------------------------------------------------------\n# ğŸ“Œ Handling Weekends as Holidays\n# --------------------------------------------------------\ncalendar[\"date\"] = pd.to_datetime(calendar[\"date\"])\nmask = (calendar[\"date\"].dt.weekday >= 5) & calendar[\"holiday_name_mapped\"].isna()\ncalendar.loc[mask, \"holiday_name_mapped\"] = \"weekend\"\nprint(calendar[\"holiday_name_mapped\"].unique())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T21:34:35.058548Z","iopub.execute_input":"2025-02-10T21:34:35.058851Z","iopub.status.idle":"2025-02-10T21:34:35.087698Z","shell.execute_reply.started":"2025-02-10T21:34:35.058826Z","shell.execute_reply":"2025-02-10T21:34:35.086618Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"calendar['any_school_holiday'] = np.where(\n    (calendar['winter_school_holidays'] + calendar['school_holidays']) > 0,\n    1,\n    0\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T21:34:35.088719Z","iopub.execute_input":"2025-02-10T21:34:35.089062Z","iopub.status.idle":"2025-02-10T21:34:35.095413Z","shell.execute_reply.started":"2025-02-10T21:34:35.089033Z","shell.execute_reply":"2025-02-10T21:34:35.094425Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# --------------------------------------------------------\n# ğŸ“Œ Adding Country Information for Warehouses\n# --------------------------------------------------------\ncalendar['country'] = calendar['warehouse'].map(warehouse_country_map)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"calendar=calendar.drop(columns=['holiday_name', 'winter_school_holidays', 'school_holidays'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T21:34:35.124454Z","iopub.execute_input":"2025-02-10T21:34:35.124740Z","iopub.status.idle":"2025-02-10T21:34:35.130714Z","shell.execute_reply.started":"2025-02-10T21:34:35.124717Z","shell.execute_reply":"2025-02-10T21:34:35.129909Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --------------------------------------------------------\n# ğŸ“Œ Creating Shift Features for Holidays and Closures\n# --------------------------------------------------------\ncalendar=calendar.sort_values(['date']).reset_index(drop=True)\nfor gap in [-1,1,2,3,4,5,6,7]:\n        for col in ['any_school_holiday', 'holiday' , 'shops_closed']:\n            calendar[col+f\"_shift{gap}\"]=calendar.groupby(['warehouse'])[col].shift(gap)\n\n\n# Ã–rnek: calendar DataFrame'inde, aÅŸaÄŸÄ±daki Ã¶nekler iÃ§in iÅŸlemi yapalÄ±m:\nprefixes = ['any_school_holiday', 'holiday', 'shops_closed']\n\nfor prefix in prefixes:\n    # shift3'ten shift7'ye kadar olan sÃ¼tun isimlerini oluÅŸturuyoruz\n    cols = [f\"{prefix}_shift{i}\" for i in range(3, 8)]\n    # SÃ¼tunlarÄ±n hepsinin mevcut olduÄŸunu kontrol edelim\n    existing_cols = [col for col in cols if col in calendar.columns]\n    if existing_cols:\n        # shift3 sÃ¼tunu varsa; yoksa oluÅŸturabiliriz\n        shift3_col = f\"{prefix}_shift3\"\n        if shift3_col in calendar.columns:\n            # Mevcut shift3 deÄŸerine diÄŸer shift sÃ¼tunlarÄ±nÄ±n toplamÄ±nÄ± ekleyelim\n            calendar[shift3_col] = calendar[existing_cols].sum(axis=1)\n        else:\n            # EÄŸer shift3 sÃ¼tunu yoksa, direkt toplamÄ± atayalÄ±m\n            calendar[shift3_col] = calendar[existing_cols].sum(axis=1)\n        # shift4'ten shift7'ye kadar olan sÃ¼tunlarÄ± sil\n        cols_to_drop = [f\"{prefix}_shift{i}\" for i in range(4, 8) if f\"{prefix}_shift{i}\" in calendar.columns]\n        calendar.drop(columns=cols_to_drop, inplace=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T21:34:35.132653Z","iopub.execute_input":"2025-02-10T21:34:35.132990Z","iopub.status.idle":"2025-02-10T21:34:35.212339Z","shell.execute_reply.started":"2025-02-10T21:34:35.132954Z","shell.execute_reply":"2025-02-10T21:34:35.211205Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --------------------------------------------------------\n# ğŸ“Œ Merge Calendar, Inventory, and Weights into the Main Dataset\n# --------------------------------------------------------\n\ndef data_frame_merge(df):\n    dt=df.merge(inventory, on=['unique_id'], how='left')\n    dt=dt.rename(columns={'warehouse_x': 'warehouse'})\n    dt[\"date\"] = pd.to_datetime(dt[\"date\"])\n    dt=dt.merge(calendar, on=['warehouse', 'date'], how='left')\n    dt=dt.merge(test_weights, on=['unique_id'], how='left')\n    \n\n    return dt\n\n\n\ntrain_merged  = data_frame_merge(combined_df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T21:34:35.213534Z","iopub.execute_input":"2025-02-10T21:34:35.214068Z","iopub.status.idle":"2025-02-10T21:34:44.777207Z","shell.execute_reply.started":"2025-02-10T21:34:35.214032Z","shell.execute_reply":"2025-02-10T21:34:44.776131Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Free memory by deleting unused variables\ndel calendar\ndel inventory\ndel sales_train\ndel sales_test\ndel solution\ndel test_weights\ndel combined_df\ndel holidays_df\ndel calendar_updated\ndel changes\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T21:34:44.786480Z","iopub.execute_input":"2025-02-10T21:34:44.786805Z","iopub.status.idle":"2025-02-10T21:34:44.857127Z","shell.execute_reply.started":"2025-02-10T21:34:44.786765Z","shell.execute_reply":"2025-02-10T21:34:44.856018Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Null Handling**","metadata":{}},{"cell_type":"code","source":"null_count = train_merged.isnull().sum()\nprint(null_count)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T21:34:44.858211Z","iopub.execute_input":"2025-02-10T21:34:44.858498Z","iopub.status.idle":"2025-02-10T21:34:46.881984Z","shell.execute_reply.started":"2025-02-10T21:34:44.858474Z","shell.execute_reply":"2025-02-10T21:34:46.880953Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.impute import KNNImputer\nfrom sklearn.preprocessing import StandardScaler\n\ndef fill_complex_nulls_by_group_iterative(df, \n                                          group_cols=['warehouse', 'unique_id'], \n                                          date_col='date', \n                                          fill_cols=None,\n                                          dist_cols=None,\n                                          n_neighbors=5, \n                                          weights=\"uniform\"):\n    \"\"\"\n    Bu fonksiyon, df iÃ§indeki fill_cols listesinde belirtilen sayÄ±sal kolonlardaki (NaN) eksikleri,\n    Ã¶nce belirtilen grup sÃ¼tunlarÄ±na gÃ¶re (Ã¶rn. warehouse, unique_id) gruplandÄ±rÄ±p, \n    her grup iÃ§inde date kolonuna gÃ¶re sÄ±raladÄ±ktan sonra, kolon kolon (iteratif) doldurur.\n    \n    Her doldurma iterasyonunda, imputation iÃ§in kullanÄ±lacak distance sÃ¼tunlarÄ±,\n    dist_cols listesinde belirtilir. EÄŸer doldurulmasÄ± istenen kolon (Ã¶rneÄŸin 'sales'),\n    aynÄ± anda distance sÃ¼tunlarÄ± arasÄ±nda yer alÄ±yorsa ve bu sÃ¼tunlardan herhangi birinde eksik deÄŸer varsa,\n    o sÃ¼tun bu iterasyonda predictor setine dahil edilmez.\n    \n    Parametreler:\n      - df: Ä°ÅŸlenecek DataFrame.\n      - group_cols: Gruplama yapÄ±lacak sÃ¼tunlar (varsayÄ±lan ['warehouse', 'unique_id']).\n      - date_col: Tarih bilgisini iÃ§eren sÃ¼tun adÄ± (varsayÄ±lan 'date').\n      - fill_cols: DoldurulmasÄ± gereken kolonlarÄ±n listesi. EÄŸer None verilirse,\n                   Ã¶rneÄŸin ['total_orders', 'sales', 'sell_price_main', 'availability'] kullanÄ±lÄ±r.\n      - dist_cols: Mesafe hesaplamasÄ±nda kullanÄ±lacak sÃ¼tunlarÄ±n listesi. Ã–rneÄŸin: \n                   ['date', 'sell_price_main', 'total_orders', 'holiday'].\n                   Burada 'date' yerine, imputation sÄ±rasÄ±nda 'date_numeric' kullanÄ±lacaktÄ±r.\n      - n_neighbors: KNN'de kullanÄ±lacak komÅŸu sayÄ±sÄ±.\n      - weights: KomÅŸu aÄŸÄ±rlÄ±klandÄ±rma yÃ¶ntemi (\"uniform\" veya \"distance\").\n    \"\"\"\n    if fill_cols is None:\n        fill_cols = ['availability', 'total_orders','sales' ]\n    if dist_cols is None:\n        dist_cols = ['date', 'sell_price_main', 'total_orders',  'availability','sales']\n    \n    # Ã–ncelikle DataFrame'i grup sÃ¼tunlarÄ± ve tarih sÃ¼tununa gÃ¶re sÄ±ralÄ±yoruz.\n    df = df.sort_values(group_cols + [date_col]).copy()\n    \n    def impute_group(group):\n        group = group.copy()\n        # Grup iÃ§indeki 'date' sÃ¼tununu, en erken tarihten itibaren geÃ§en gÃ¼n sayÄ±sÄ±na Ã§evirip 'date_numeric' olarak ekleyelim.\n        group['date_numeric'] = (group[date_col] - group[date_col].min()).dt.days\n        \n        # Her fill kolonu iÃ§in iterasyon (sÄ±rasÄ±yla doldurma)\n        for col in fill_cols:\n            missing_idx = group[group[col].isnull()].index\n            if len(missing_idx) == 0:\n                continue  # Bu kolonda eksik yoksa geÃ§\n            \n            # Ä°lk olarak, doldurulmasÄ± istenen kolonu distance sÃ¼tunlarÄ± arasÄ±ndan Ã§Ä±kartÄ±yoruz.\n            predictors = [p for p in dist_cols if p != col]\n            # 'date' sÃ¼tunu varsa, onun yerine 'date_numeric' kullanÄ±lacak.\n            predictors = ['date_numeric' if p == 'date' else p for p in predictors]\n            \n            # Åimdi, eÄŸer ilgili grup iÃ§inde herhangi bir predictor sÃ¼tununda (predictors listesinde)\n            # eksik deÄŸer varsa, o sÃ¼tunu predictor setinden Ã§Ä±karÄ±yoruz.\n            predictors = [p for p in predictors if not group[p].isnull().any()]\n            \n            # Ä°mputation iÃ§in kullanÄ±lacak sÃ¼tun setini belirleyelim: doldurulmasÄ± istenen kolon + predictors.\n            impute_features = [col] + predictors\n            # EÄŸer bazÄ± impute_features grupta yoksa, onlarÄ± Ã§Ä±karalÄ±m.\n            impute_features = [feat for feat in impute_features if feat in group.columns]\n            \n            # Alt DataFrame oluÅŸturup, sadece bu sÃ¼tunlarÄ± seÃ§elim.\n            data = group[impute_features]\n            \n            # Ã–lÃ§ekleme: TÃ¼m sÃ¼tunlarÄ± aynÄ± Ã¶lÃ§eÄŸe getiriyoruz.\n            scaler = StandardScaler()\n            data_scaled = scaler.fit_transform(data)\n            \n            # KNN imputer ile eksik deÄŸerleri dolduralÄ±m.\n            imputer = KNNImputer(n_neighbors=n_neighbors, weights=weights)\n            data_imputed_scaled = imputer.fit_transform(data_scaled)\n            data_imputed = scaler.inverse_transform(data_imputed_scaled)\n            \n            # Ä°mputed veriyi DataFrame olarak elde edelim.\n            data_imputed_df = pd.DataFrame(data_imputed, columns=impute_features, index=group.index)\n            \n            # Sadece doldurulmasÄ± istenen kolon iÃ§in imputed deÄŸerleri, eksik olan yerlerde gÃ¼ncelleyelim.\n            group.loc[missing_idx, col] = data_imputed_df.loc[missing_idx, col]\n        \n        # Ä°ÅŸlem sonunda geÃ§ici eklenen 'date_numeric' sÃ¼tununu kaldÄ±rÄ±yoruz.\n        group.drop(columns=['date_numeric'], inplace=True)\n        return group\n    \n    # Gruplama iÅŸlemi: Her grup iÃ§in imputation uyguluyoruz.\n    df_imputed = df.groupby(group_cols, group_keys=False).apply(impute_group)\n    return df_imputed\n\n# Ã–rnek kullanÄ±m:\n# VarsayalÄ±m ki train_merged DataFrame'inizde 'warehouse', 'unique_id', 'date' gibi sÃ¼tunlar ve doldurulmasÄ± istenen\n# sayÄ±sal kolonlar (Ã¶rneÄŸin 'total_orders', 'sales', 'sell_price_main', 'availability') mevcut.\nfill_columns = ['availability', 'total_orders','sales' ]\ngroup_columns = ['warehouse', 'unique_id']\ndate_column = 'date'\n# Distance hesaplamasÄ±nda kullanÄ±lacak sÃ¼tunlar:\ndistance_columns = ['date', 'sell_price_main', 'total_orders', 'availability','sales']\n\n# Fonksiyonu Ã§aÄŸÄ±rÄ±yoruz:\ntrain_merged = fill_complex_nulls_by_group_iterative(train_merged, \n                                                     group_cols=group_columns, \n                                                     date_col=date_column, \n                                                     fill_cols=fill_columns, \n                                                     dist_cols=distance_columns, \n                                                     n_neighbors=5, \n                                                     weights=\"uniform\")\n\n# SonuÃ§: Her grup iÃ§inde, sÄ±ralamaya gÃ¶re (tarih) ve belirlenen distance sÃ¼tunlarÄ±na gÃ¶re\n# eksik deÄŸerler, iteratif olarak (kolon bazÄ±nda) doldurulmuÅŸ olacaktÄ±r.\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T21:34:46.883000Z","iopub.execute_input":"2025-02-10T21:34:46.883256Z","iopub.status.idle":"2025-02-10T21:36:05.898709Z","shell.execute_reply.started":"2025-02-10T21:34:46.883233Z","shell.execute_reply":"2025-02-10T21:36:05.897518Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Outlier Handling**","metadata":{}},{"cell_type":"code","source":"# I want to see outliers here\n\"\"\"\nfrom sklearn.ensemble import IsolationForest\n\n# Sadece sayÄ±sal sÃ¼tunlarla Ã§alÄ±ÅŸ\nnumeric_cols = ['sales', 'total_orders', 'sell_price_main', 'availability']  # Ã–rnek\nX = train_merged[numeric_cols].dropna()\n\n# Isolation Forest modeli\niso_forest = IsolationForest(n_estimators=100, contamination=0.02, random_state=42)\npreds = iso_forest.fit_predict(X)\n\n# AykÄ±rÄ± deÄŸerler (-1)\naykiri_veriler = train_merged[preds == -1]\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T21:36:05.900224Z","iopub.execute_input":"2025-02-10T21:36:05.900656Z","iopub.status.idle":"2025-02-10T21:36:05.906930Z","shell.execute_reply.started":"2025-02-10T21:36:05.900629Z","shell.execute_reply":"2025-02-10T21:36:05.906027Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import IsolationForest\nfrom scipy.stats.mstats import winsorize\nfrom sklearn.preprocessing import StandardScaler\n\ndef detect_and_correct_anomalies(dataframe, columns_to_check):\n    \"\"\"\n    Advanced anomaly detection and correction pipeline\n    \n    Parameters:\n    - dataframe: Input DataFrame\n    - columns_to_check: Numeric columns for anomaly detection\n    \n    Returns:\n    - Processed DataFrame with anomaly corrections\n    \"\"\"\n    \n    # Create a copy of the DataFrame to prevent modifications\n    df_processed = dataframe.copy()\n    \n    # 1. Prepare data for anomaly detection\n    data_for_detection = df_processed[columns_to_check].copy()\n    \n    # Handle any remaining missing values\n    data_for_detection.fillna(data_for_detection.median(), inplace=True)\n    \n    # 2. Standardize data for better anomaly detection\n    scaler = StandardScaler()\n    scaled_data = scaler.fit_transform(data_for_detection)\n    \n    # 3. Anomaly Detection using Isolation Forest\n    iso_forest = IsolationForest(\n        contamination=0.0001,  # 2% of data considered anomalous\n        random_state=42,\n        max_samples='auto',\n        bootstrap=False\n    )\n    \n    # Detect anomalies\n    anomaly_labels = iso_forest.fit_predict(scaled_data)\n    \n    # 4. Add anomaly flag to DataFrame\n    df_processed['is_anomaly'] = anomaly_labels == -1\n    \n    # 5. Detailed Anomaly Analysis\n    def calculate_anomaly_statistics(df, columns):\n        anomaly_stats = {}\n        for col in columns:\n            anomaly_stats[col] = {\n                'total_anomalies': df[df['is_anomaly']][col].count(),\n                'anomaly_percentage': (df['is_anomaly'].sum() / len(df)) * 100,\n                'original_mean': df[col].mean(),\n                'anomaly_mean': df[df['is_anomaly']][col].mean(),\n                'original_std': df[col].std(),\n                'anomaly_std': df[df['is_anomaly']][col].std()\n            }\n        return anomaly_stats\n    \n    anomaly_statistics = calculate_anomaly_statistics(df_processed, columns_to_check)\n    \n    # 6. Winsorization for Anomaly Correction\n    def winsorize_columns(df, columns, limits=(0.05, 0.05)):\n        df_winsorized = df.copy()\n        for col in columns:\n            df_winsorized[col] = winsorize(df_winsorized[col], limits=limits)\n        return df_winsorized\n    \n    # Apply Winsorization\n    df_winsorized = winsorize_columns(df_processed, columns_to_check)\n    \n    # 7. Correction Strategy\n    # Option 1: Replace anomalies with winsorized values\n    for col in columns_to_check:\n        df_processed.loc[df_processed['is_anomaly'], col] = df_winsorized.loc[df_processed['is_anomaly'], col]\n    \n    # 8. Additional Validation\n    def print_anomaly_report(stats):\n        print(\"Anomaly Detection Report:\")\n        for col, stat in stats.items():\n            print(f\"\\nColumn: {col}\")\n            print(f\"Total Anomalies: {stat['total_anomalies']}\")\n            print(f\"Anomaly Percentage: {stat['anomaly_percentage']:.2f}%\")\n            print(f\"Original Mean: {stat['original_mean']:.2f}\")\n            print(f\"Anomaly Mean: {stat['anomaly_mean']:.2f}\")\n    \n    print_anomaly_report(anomaly_statistics)\n    \n    return df_processed\n\n# Example Usage\ncolumns_to_analyze = [\n    'sales'\n]\n\n#outlier handling does not applied since it didn't improve results\n# Apply anomaly detection and correction\ntrain_merged_corrected = train_merged\n\n\"\"\"\n# Validation\nprint(\"\\nOriginal Train DataFrame Shape:\", train_merged.shape)\nprint(\"Corrected Train DataFrame Shape:\", train_merged_corrected.shape)\nprint(\"\\nAnomaly Detection Summary (Train):\")\nprint(\"Total Anomalies Detected:\", train_merged_corrected['is_anomaly'].sum())\n\"\"\"\n\ndel train_merged\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T21:36:05.907962Z","iopub.execute_input":"2025-02-10T21:36:05.908351Z","iopub.status.idle":"2025-02-10T21:36:05.954662Z","shell.execute_reply.started":"2025-02-10T21:36:05.908306Z","shell.execute_reply":"2025-02-10T21:36:05.953639Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Feature Engineering**","metadata":{}},{"cell_type":"markdown","source":"**Date Features**","metadata":{}},{"cell_type":"code","source":"\n\n\n\n# ğŸ“Œ Tarih formatÄ±nÄ± dÃ¼zelt ve sÄ±rala\ntrain_merged_corrected['date'] = pd.to_datetime(train_merged_corrected['date'])\ntrain_merged_corrected = train_merged_corrected.sort_values('date')\n\n\n# Tarihi bileÅŸenlerine ayÄ±rma\ntrain_merged_corrected['date'] = pd.to_datetime(train_merged_corrected['date'])\ntrain_merged_corrected['year'] = train_merged_corrected['date'].dt.year\ntrain_merged_corrected['month'] = train_merged_corrected['date'].dt.month\ntrain_merged_corrected['day'] = train_merged_corrected['date'].dt.day\ntrain_merged_corrected['day_of_week'] = train_merged_corrected['date'].dt.dayofweek\ntrain_merged_corrected['is_weekend'] = train_merged_corrected['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)\ntrain_merged_corrected['week_of_year'] = train_merged_corrected['date'].dt.isocalendar().week\ntrain_merged_corrected['day_of_year']=train_merged_corrected['date'].dt.dayofyear\ntrain_merged_corrected['quarter'] = train_merged_corrected['date'].dt.quarter\n\n\n# --------------------------------------------------------\n# ğŸ“Œ Creating Cyclical Features Using Sine and Cosine Transformations\n# --------------------------------------------------------\ntrain_merged_corrected['month_sin'] = np.sin(2 * np.pi * train_merged_corrected['month'] / 12) \ntrain_merged_corrected['month_cos'] = np.cos(2 * np.pi * train_merged_corrected['month'] / 12)\ntrain_merged_corrected['day_sin'] = np.sin(2 * np.pi * train_merged_corrected['day'] / 31)  \ntrain_merged_corrected['day_cos'] = np.cos(2 * np.pi * train_merged_corrected['day'] / 31)\ntrain_merged_corrected['sin_week']=np.sin(2*np.pi*train_merged_corrected['week_of_year']/52)\ntrain_merged_corrected['cos_week']=np.cos(2*np.pi*train_merged_corrected['week_of_year']/52)\ntrain_merged_corrected['sin_day_of_year']=np.sin(2*np.pi*train_merged_corrected['day_of_year']/365)\ntrain_merged_corrected['cos_day_of_year']=np.cos(2*np.pi*train_merged_corrected['day_of_year']/365)\ntrain_merged_corrected['day_of_week_sin'] = np.sin(\n    2 * np.pi * train_merged_corrected['day_of_week'] / 7\n)\ntrain_merged_corrected['day_of_week_cos'] = np.cos(\n    2 * np.pi * train_merged_corrected['day_of_week'] / 7\n)\ntrain_merged_corrected['quarter_sin'] = np.sin(\n    2 * np.pi * (train_merged_corrected['quarter']) / 4\n)\ntrain_merged_corrected['quarter_cos'] = np.cos(\n    2 * np.pi * (train_merged_corrected['quarter']) / 4\n)\n\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_merged_corrected=train_merged_corrected.drop(columns='warehouse_y')\ntrain_merged_corrected.info()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Trend Features**","metadata":{}},{"cell_type":"code","source":"\n\n# --------------------------------------------------------\n# ğŸ“Œ Time Series Trend Features (Lag & Rolling Mean Features)\n# --------------------------------------------------------\ntrain_merged_corrected = train_merged_corrected.sort_values(by=['unique_id', 'warehouse', 'date'])\ndef create_time_features(df, windows=[7,14,20,28,35,84,356], group_cols=['unique_id', 'warehouse'], target_col='sales'):\n    \"\"\"\n    Create lag and rolling mean features for specified windows\n    Parameters:\n    df: pandas DataFrame\n    windows: list of integers representing the window sizes\n    group_cols: list of columns to group by\n    target_col: column to create features from\n    \n    Returns:\n    pandas DataFrame with new features added\n    \"\"\"\n    result_df = df.copy()\n    result_df = result_df.sort_values(by=['unique_id', 'warehouse', 'date'])\n    # Create lag features\n    for window in windows:\n        result_df[f'{target_col}_lag_{window}'] = (\n            result_df.groupby(group_cols)[target_col]\n            .shift(window)\n        )\n    # Create rolling mean features\n    for window in windows:\n        result_df[f'{target_col}_rolling_mean_{window}'] = (\n            result_df.groupby(group_cols)[target_col]\n            .rolling(window=window, min_periods=1)\n            .mean()\n            .reset_index(level=list(range(len(group_cols))), drop=True)\n        )\n    return result_df\n# Uygula\nwindows = [7,14,20,28,35,84,356]\ntrain_merged_corrected = create_time_features(\n    df=train_merged_corrected,\n    windows=windows,\n    group_cols=['unique_id', 'warehouse'],\n    target_col='sales'\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Sales and Prices Features**","metadata":{}},{"cell_type":"code","source":"\n\n# ğŸ“Œ max sales\ntrain_merged_corrected['max_sales'] = (\n    train_merged_corrected.groupby(['unique_id', 'warehouse'])['sales']\n    .transform('max')\n)\n\n\n\n\ndiscount_columns = ['type_0_discount', 'type_1_discount', 'type_2_discount', 'type_3_discount', \n                    'type_4_discount', 'type_5_discount', 'type_6_discount']\n\ntrain_merged_corrected['max_discount'] = train_merged_corrected[discount_columns].max(axis=1)\ntrain_merged_corrected['total_discount'] = train_merged_corrected[discount_columns].apply(\n    lambda x: x[x >= 0].sum(), axis=1\n)\n\n# **Price Change (Fiyat DeÄŸiÅŸimi)**\ntrain_merged_corrected['price_change'] = train_merged_corrected.groupby(['unique_id', 'warehouse'])['sell_price_main'].pct_change()\n\n\n\n# ğŸ“Œ 'unique_id' kolonundaki alt Ã§izgiden Ã¶nceki kÄ±smÄ± Ã§Ä±kar\ntrain_merged_corrected['base_unique_id'] = train_merged_corrected['product_unique_id'].astype(str).str.split('_').str[0]\n\n# ğŸ“Œ Yeni 'base_unique_id' bazÄ±nda ortalama fiyat hesapla\naverage_price_df = (\n    train_merged_corrected.groupby('base_unique_id')['sell_price_main']\n    .mean()\n    .reset_index()\n    .rename(columns={'sell_price_main': 'avg_sell_price_main'})\n)\n\n# ğŸ“Œ Ana veri setine ortalama fiyatÄ± ekleme (Join iÅŸlemi)\ntrain_merged_corrected = train_merged_corrected.merge(average_price_df, on='base_unique_id', how='left')\n\n\n\ntrain_merged_corrected['product_name'] = train_merged_corrected['name'].str.split('_').str[0]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**GDP**","metadata":{}},{"cell_type":"code","source":"#deactivated because external data is prohibited\n\"\"\"\nimport wbdata\nimport pandas as pd\nfrom datetime import datetime\n\n# 1. Warehouse sÃ¼tunundaki ÅŸehirleri Ã¼lkelere eÅŸleÅŸtirme\nwarehouse_to_country = {\n    \"Budapest_1\": \"Hungary\",\n    \"Prague_2\": \"Czechia\",\n    \"Brno_1\": \"Czechia\",\n    \"Prague_1\": \"Czechia\",\n    \"Prague_3\": \"Czechia\",\n    \"Munich_1\": \"Germany\",\n    \"Frankfurt_1\": \"Germany\"\n}\n# Ãœlke isimlerini ekleyelim\ntrain_merged_corrected[\"country\"] = train_merged_corrected[\"warehouse\"].map(warehouse_to_country)\n\n# 2. DÃ¼nya BankasÄ± gÃ¶stergesi: GSYÄ°H (GDP)\nindicator = {\"NY.GDP.MKTP.CD\": \"GDP\"}\n\n# 3. GSYÄ°H verilerini Ã§ekme (convert_date kaldÄ±rÄ±ldÄ±)\ngdp_data = wbdata.get_dataframe(indicator)\n\n# 4. Ä°ndeksi sÄ±fÄ±rlayÄ±p 'country' ve 'date' sÃ¼tunlarÄ±nÄ± elde ediyoruz.\ngdp_data.reset_index(inplace=True)  \n# gdp_data sÃ¼tunlarÄ± Ã¶rneÄŸin: 'country', 'date', 'GDP' ÅŸeklinde olacak.\n\n# 5. 'date' sÃ¼tununu datetime formatÄ±na Ã§evirip 'year' sÃ¼tununu oluÅŸturuyoruz.\ngdp_data['date'] = pd.to_datetime(gdp_data['date'])\ngdp_data['year'] = gdp_data['date'].dt.year\n\n# 6. Gdp verilerinde kullanÄ±lan 'year' sÃ¼tununun tipini int64'e dÃ¶nÃ¼ÅŸtÃ¼relim.\ngdp_data['year'] = gdp_data['year'].astype('int64')\n\n# 7. Train veri setinde, her satÄ±r iÃ§in bir Ã¶nceki yÄ±lÄ± belirten \"prev_year\" sÃ¼tununu oluÅŸturuyoruz.\ntrain_merged_corrected[\"prev_year\"] = train_merged_corrected[\"year\"] - 1\n\n# 8. Train veri setindeki 'year' ve 'prev_year' sÃ¼tunlarÄ±nÄ±n tipini int64 yapalÄ±m.\ntrain_merged_corrected[\"year\"] = train_merged_corrected[\"year\"].astype('int64')\ntrain_merged_corrected[\"prev_year\"] = train_merged_corrected[\"prev_year\"].astype('int64')\n\n# 9. GSYÄ°H verilerini, Ã¼lke ve \"prev_year\" bilgisi Ã¼zerinden birleÅŸtiriyoruz.\n#    Merge iÅŸlemi sol tarafta \"prev_year\" ile, saÄŸ tarafta \"year\" sÃ¼tunu Ã¼zerinden gerÃ§ekleÅŸecek.\ntrain_merged_corrected = train_merged_corrected.merge(\n    gdp_data,\n    how=\"left\",\n    left_on=[\"country\", \"prev_year\"],\n    right_on=[\"country\", \"year\"]\n)\n\n# 10. Merge sonrasÄ± gelen GDP sÃ¼tununu \"GDP_prev\" olarak yeniden adlandÄ±rÄ±yoruz.\ntrain_merged_corrected.rename(columns={\"year_x\": \"year\"}, inplace=True)\ntrain_merged_corrected.rename(columns={\"date_x\": \"date\"}, inplace=True)\n\ntrain_merged_corrected.drop(columns=[  \"year_y\"], errors=\"ignore\", inplace=True)\n\n# SonuÃ§ kontrolÃ¼:\nprint(train_merged_corrected[['warehouse', 'year', 'prev_year', 'country', 'GDP']].head())\n\"\"\"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef create_statistical_features_optimized(df, target_col, group_cols=['unique_id', 'warehouse']):\n    \"\"\"\n    Verilen veri seti Ã¼zerinde, group_cols bazÄ±nda target_col iÃ§in Ã§eÅŸitli istatistiksel Ã¶znitelikler oluÅŸturur.\n    Tek bir groupby-aggregation ile hesaplanan Ã¶znitelikler, orijinal df'e merge edilir.\n\n    Parameters:\n      df: pandas DataFrame\n      group_cols: gruplama iÃ§in kullanÄ±lacak sÃ¼tunlar\n      target_col: istatistiklerin hesaplanacaÄŸÄ± hedef sÃ¼tun\n\n    Returns:\n      Yeni Ã¶zniteliklerle geniÅŸletilmiÅŸ pandas DataFrame\n    \"\"\"\n    # Orijinal veri setini kopyalÄ±yoruz.\n    result_df = df.copy()\n\n    # 1) Tek seferde groupby-agg sÃ¶zlÃ¼ÄŸÃ¼\n    agg_dict = {\n        f'{target_col}_max': 'max',\n        f'{target_col}_mean': 'mean',\n        f'{target_col}_median': 'median',\n        f'{target_col}_std': 'std',\n        f'{target_col}_skew': 'skew',\n        f'{target_col}_zero_ratio': lambda x: (x == 0).mean()\n    }\n\n    # 2) groupby ile Ã¶zet tablomuza Ã§eviriyoruz\n    agg_df = (\n        result_df\n        .groupby(group_cols)[target_col]\n        .agg(**agg_dict)\n        .reset_index()\n    )\n\n\n    # 4) Orijinal df ile hesaplanan Ã¶zellikleri merge\n    result_df = result_df.merge(agg_df, on=group_cols, how='left')\n\n    return result_df\n\n# Ã–rnek kullanÄ±m\nfor col in ['sales', 'total_orders', 'sell_price_main', 'total_discount']:\n    train_merged_corrected = create_statistical_features_optimized(\n        df=train_merged_corrected,\n        target_col=col,\n        group_cols=['unique_id', 'warehouse']\n    )\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef reduce_mem_usage(df):\n    start_mem = df.memory_usage().sum() / 1024**2\n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type not in [object, 'category', 'datetime64[ns]']:\n            c_min = df[col].min()\n            c_max = df[col].max()\n\n            # tamsayÄ± sÃ¼tunlar\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                else:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                # float sÃ¼tunlar\n                df[col] = df[col].astype(np.float32)\n                \n        elif col_type == object:\n            # EÄŸer gerÃ§ekte kategorik veya sayÄ±sal deÄŸilse, kategorik dÃ¶nÃ¼ÅŸtÃ¼rebilirsiniz\n            # df[col] = df[col].astype('category')\n            pass\n    \n    end_mem = df.memory_usage().sum() / 1024**2\n    print(f\"Bellek kullanÄ±mÄ±: {start_mem:.2f} MB -> {end_mem:.2f} MB\")\n    return df\ntrain_merged_corrected=  reduce_mem_usage  (train_merged_corrected)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\ndef create_statistical_features_optimized2(df, target_col, group_cols=['unique_id', 'warehouse','year']):\n    \"\"\"\n    Verilen veri seti Ã¼zerinde, group_cols bazÄ±nda target_col iÃ§in Ã§eÅŸitli istatistiksel Ã¶znitelikler oluÅŸturur.\n    Tek bir groupby-aggregation ile hesaplanan Ã¶znitelikler, orijinal df'e merge edilir.\n\n    Parameters:\n      df: pandas DataFrame\n      group_cols: gruplama iÃ§in kullanÄ±lacak sÃ¼tunlar\n      target_col: istatistiklerin hesaplanacaÄŸÄ± hedef sÃ¼tun\n\n    Returns:\n      Yeni Ã¶zniteliklerle geniÅŸletilmiÅŸ pandas DataFrame\n    \"\"\"\n    # Orijinal veri setini kopyalÄ±yoruz.\n    result_df = df.copy()\n\n    # 1) Tek seferde groupby-agg sÃ¶zlÃ¼ÄŸÃ¼\n    agg_dict = {\n        f'{target_col}_yearly_max': 'max',\n        f'{target_col}_yearly_min': 'min',\n        f'{target_col}_yearly_mean': 'mean',\n        f'{target_col}_yearly_median': 'median',\n        f'{target_col}_yearly_std': 'std',\n        f'{target_col}_yearly_skew': 'skew',\n        f'{target_col}_yearly_q25': lambda x: x.quantile(0.25),\n        f'{target_col}_yearly_q75': lambda x: x.quantile(0.75),\n        f'{target_col}_yearly_zero_ratio': lambda x: (x == 0).mean()\n    }\n\n    # 2) groupby ile Ã¶zet tablomuza Ã§eviriyoruz\n    agg_df = (\n        result_df\n        .groupby(group_cols)[target_col]\n        .agg(**agg_dict)\n        .reset_index()\n    )\n\n    # 3) CV (Coefficient of Variation) = std / mean\n    #    mean=0 durumunda bÃ¶lme hatasÄ± almamak iÃ§in 0'larÄ± NaN'a Ã§eviriyoruz\n    agg_df[f'{target_col}_yearly_cv'] = (\n        agg_df[f'{target_col}_yearly_std'] / agg_df[f'{target_col}_yearly_mean'].replace(0, np.nan)\n    )\n\n    # 4) Orijinal df ile hesaplanan Ã¶zellikleri merge\n    result_df = result_df.merge(agg_df, on=group_cols, how='left')\n\n    return result_df\n\n# Ã–rnek kullanÄ±m\nfor col in ['sales', 'total_orders', 'sell_price_main', 'availability', 'total_discount']:\n    train_merged_corrected = create_statistical_features_optimized2(\n        df=train_merged_corrected,\n        target_col=col,\n        group_cols=['unique_id', 'warehouse','year']\n    )\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# Tarih sÃ¼tununu datetime formatÄ±na Ã§eviriyoruz.\ntrain_merged_corrected['date'] = pd.to_datetime(train_merged_corrected['date'])\ntrain_merged_corrected['holiday_name'] = train_merged_corrected['holiday_name_mapped'].fillna(\"nan\")\n# GeÃ§ersiz tatil deÄŸerleri\ninvalid_values = ['weekend', 'nan']\n\ndef assign_holiday_columns_vectorized(group):\n    # Her warehouse iÃ§in tarih sÄ±ralamasÄ±\n    group = group.sort_values('date').reset_index(drop=True)\n    # Sadece geÃ§erli tatilleri filtreleyelim\n    valid = group.loc[(~group['holiday_name'].isin(invalid_values)) & (group['holiday_name'].notna())].copy()\n    \n    # EÄŸer geÃ§erli tatil yoksa, ilgili sÃ¼tunlarÄ± NaN ile dolduruyoruz.\n    if valid.empty:\n        group['next_5_days_holidays'] = np.nan\n        group['next_15_days_holidays'] = np.nan\n        group['prev_5_days_holidays'] = np.nan\n        return group\n    \n    # --- SONRAKÄ° TATÄ°LLER Ä°Ã‡Ä°N ---\n    # Arama iÃ§in: mevcut tarihe 1 gÃ¼n ekleyelim.\n    group['search_date'] = group['date'] + pd.Timedelta(days=1)\n    # merge_asof ile, search_date'e en yakÄ±n (ilerideki) geÃ§erli tatili buluyoruz.\n    merged_next = pd.merge_asof(\n        group.sort_values('search_date'),\n        valid.sort_values('date'),\n        left_on='search_date',\n        right_on='date',\n        direction='forward',\n        suffixes=('', '_hol')\n    )\n    merged_next = merged_next.sort_index()\n    \n    # VektÃ¶rize koÅŸullu atamalar:\n\n    group['next_5_days_holidays'] = np.where(\n        merged_next['date_hol'] <= group['date'] + pd.Timedelta(days=5),\n        merged_next['holiday_name_hol'], np.nan\n    )\n    group['next_15_days_holidays'] = np.where(\n        merged_next['date_hol'] <= group['date'] + pd.Timedelta(days=15),\n        merged_next['holiday_name_hol'], np.nan\n    )\n    \n    # --- Ã–NCEKÄ° TATÄ°LLER Ä°Ã‡Ä°N ---\n    # Arama iÃ§in: mevcut tarihten 1 gÃ¼n Ã§Ä±karalÄ±m.\n    group['search_date_prev'] = group['date'] - pd.Timedelta(days=1)\n    # merge_asof ile, search_date_prev'e en yakÄ±n (gerideki) geÃ§erli tatili buluyoruz.\n    merged_prev = pd.merge_asof(\n        group.sort_values('search_date_prev'),\n        valid.sort_values('date'),\n        left_on='search_date_prev',\n        right_on='date',\n        direction='backward',\n        suffixes=('', '_hol')\n    )\n    merged_prev = merged_prev.sort_index()\n    \n\n    group['prev_5_days_holidays'] = np.where(\n        merged_prev['date_hol'] >= group['date'] - pd.Timedelta(days=5),\n        merged_prev['holiday_name_hol'], np.nan\n    )\n\n    \n    # GeÃ§ici sÃ¼tunlarÄ± temizliyoruz.\n    group.drop(columns=['search_date', 'search_date_prev'], inplace=True)\n    \n    return group\n\n# Her warehouse iÃ§in gruplandÄ±rarak vektÃ¶rize iÅŸlemi uyguluyoruz.\ntrain_merged_corrected = train_merged_corrected.groupby('warehouse', group_keys=False).apply(assign_holiday_columns_vectorized)\n\n# SonuÃ§larÄ± kontrol edelim.\nprint(train_merged_corrected.head())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Ã–rnek pandemi dÃ¶nemlerini tanÄ±mlayan fonksiyon:\ndef get_pandemic_phase(date):\n    \"\"\"\n    Bu fonksiyon, verilen date deÄŸeri iÃ§in pandemi dÃ¶nemini belirler.\n    \n    Ã–rnek dÃ¶nemler:\n      - \"pre-pandemic\": 11 Mart 2020 Ã¶ncesi\n      - \"pandemic\": 11 Mart 2020 - 31 AralÄ±k 2021\n      - \"post-pandemic\": 1 Ocak 2022 - 31 MayÄ±s 2022\n      - \"healthy\": 1 Haziran 2022 sonrasÄ±\n      \n    Bu dÃ¶nemler, analiz ihtiyaÃ§larÄ±nÄ±za gÃ¶re yeniden dÃ¼zenlenebilir.\n    \"\"\"\n    if date < pd.Timestamp(\"2020-03-11\"):\n        return \"pre-pandemic\"\n    elif date <= pd.Timestamp(\"2021-12-31\"):\n        return \"pandemic\"\n    elif date <= pd.Timestamp(\"2022-05-31\"):\n        return \"post-pandemic\"\n    else:\n        return \"healthy\"\n\n# Tarih sÃ¼tununu zaten datetime formatÄ±na Ã§evirdiÄŸinizi varsayarsak:\ntrain_merged_corrected[\"pandemic_phase\"] = train_merged_corrected[\"date\"].apply(get_pandemic_phase)\n\n# Ã–rnek olarak ilk 5 satÄ±rÄ± gÃ¶rÃ¼ntÃ¼leyelim:\nprint(train_merged_corrected[[\"date\", \"pandemic_phase\"]].head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Reset index to ensure uniqueness\ntrain_merged_corrected = train_merged_corrected.reset_index(drop=True)\ntrain_merged_corrected = train_merged_corrected.sort_values(['warehouse','date'])\n\n# Now, create the 'next_shops_closed_date' column:\ntrain_merged_corrected['next_shops_closed_date'] = train_merged_corrected.loc[train_merged_corrected['shops_closed'] == 1, 'date'].shift(-1)\ntrain_merged_corrected['next_shops_closed_date'] = train_merged_corrected['next_shops_closed_date'].bfill()\ntrain_merged_corrected['days_to_next_closed'] = (train_merged_corrected['next_shops_closed_date'] - train_merged_corrected['date']).dt.days\n\n# Create a binary column indicating if the day immediately after a closed day (shops_closed==1) is open:\ntrain_merged_corrected['day_after_closing'] = ((train_merged_corrected['shops_closed'] == 0) & (train_merged_corrected['shops_closed'].shift(1) == 1)).astype(int)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Her grup iÃ§in flag hesaplama: 1500, 5200 ve 10000 eÅŸiÄŸine gÃ¶re holiday koÅŸullu ve koÅŸulsuz flag'lar\nflag_df = train_merged_corrected.groupby(['warehouse', 'unique_id']).apply(\n    lambda df: pd.Series({\n        'high_sales_1500_holidaysiz': int((df['sales'] > 1500).any()),\n        'high_sales_5200_holidaysiz': int((df['sales'] > 5200).any()),\n        'high_sales_10000_holidaysiz': int((df['sales'] > 10000).any())\n    })\n).reset_index()\n\n# Orijinal veri setine flag'larÄ± merge edelim\ntrain_merged_corrected = train_merged_corrected.merge(flag_df, on=['warehouse', 'unique_id'], how='left')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ncolumns_to_drop = [\n    'unique_id', 'date'\n]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"del average_price_df\ndel flag_df\ntrain_merged_corrected=train_merged_corrected.drop(columns=['holiday_name',\n 'next_shops_closed_date', 'base_unique_id','day_after_closing', 'max_sales'])\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T05:54:15.562474Z","iopub.execute_input":"2025-02-11T05:54:15.562838Z","iopub.status.idle":"2025-02-11T05:54:15.647692Z","shell.execute_reply.started":"2025-02-11T05:54:15.562796Z","shell.execute_reply":"2025-02-11T05:54:15.646264Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\ndef reduce_mem_usage(df):\n    start_mem = df.memory_usage().sum() / 1024**2\n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type not in [object, 'category', 'datetime64[ns]']:\n            c_min = df[col].min()\n            c_max = df[col].max()\n\n            # tamsayÄ± sÃ¼tunlar\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                else:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                # float sÃ¼tunlar\n                df[col] = df[col].astype(np.float32)\n                \n        elif col_type == object:\n            # EÄŸer gerÃ§ekte kategorik veya sayÄ±sal deÄŸilse, kategorik dÃ¶nÃ¼ÅŸtÃ¼rebilirsiniz\n            # df[col] = df[col].astype('category')\n            pass\n    \n    end_mem = df.memory_usage().sum() / 1024**2\n    print(f\"Bellek kullanÄ±mÄ±: {start_mem:.2f} MB -> {end_mem:.2f} MB\")\n    return df\ntrain_merged_corrected=  reduce_mem_usage  (train_merged_corrected)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Export without index for easier reading later\ntrain_merged_corrected.to_csv(\"train_merged_corrected.csv\", index=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}