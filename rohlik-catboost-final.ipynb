{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":88742,"databundleVersionId":10173359,"sourceType":"competition"},{"sourceId":222390171,"sourceType":"kernelVersion"}],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This script loads and processes sales data, performs feature engineering,\n# optimizes memory usage, and trains a CatBoost model for forecasting sales.\n# The model works with pre-set hyperparameters (optimized in another notebook) and preprocessed data(processed in another notebook)\n\n# Import necessary libraries\nimport numpy as np  # For linear algebra operations\nimport pandas as pd  # For data processing, CSV file I/O (e.g. pd.read_csv)\nimport gc  # For garbage collection to free up memory\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-13T07:14:32.296169Z","iopub.execute_input":"2025-02-13T07:14:32.296494Z","iopub.status.idle":"2025-02-13T07:14:32.729172Z","shell.execute_reply.started":"2025-02-13T07:14:32.296469Z","shell.execute_reply":"2025-02-13T07:14:32.727946Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# LOAD DATA","metadata":{}},{"cell_type":"code","source":"# Load sales training and test data\nsales_train = pd.read_csv('/kaggle/input/rohlik-sales-forecasting-challenge-v2/sales_train.csv')\nsales_test = pd.read_csv('/kaggle/input/rohlik-sales-forecasting-challenge-v2/sales_test.csv')\n\n# Convert date columns to datetime format\nsales_train['date'] = pd.to_datetime(sales_train['date'])\nsales_test['date'] = pd.to_datetime(sales_test['date'])\n\n# Print min and max dates for train and test sets\ntrain_min_date = sales_train['date'].min()\ntrain_max_date = sales_train['date'].max()\ntest_min_date = sales_test['date'].min()\ntest_max_date = sales_test['date'].max()\nprint(\"Sales Train - En Küçük Tarih:\", train_min_date)\nprint(\"Sales Train - En Büyük Tarih:\", train_max_date)\nprint(\"Sales Test - En Küçük Tarih:\", test_min_date)\nprint(\"Sales Test - En Büyük Tarih:\", test_max_date)\n\n# Delete the loaded sales_train and sales_test DataFrames to free memory\ndel sales_train\ndel sales_test\n\n# Run garbage collection to reclaim memory from deleted objects\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T07:14:32.730736Z","iopub.execute_input":"2025-02-13T07:14:32.731351Z","iopub.status.idle":"2025-02-13T07:14:41.377248Z","shell.execute_reply.started":"2025-02-13T07:14:32.731308Z","shell.execute_reply":"2025-02-13T07:14:41.376321Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the pre-processed training dataset\ntrain_merged_corrected = pd.read_csv('/kaggle/input/rohlik-dataprep/train_merged_corrected.csv', low_memory=True)\n\n# Optimize display settings for better visualization\npd.set_option('display.max_columns', None)  # Show all columns\npd.set_option('display.max_rows', None)  # Show all rows (optional)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T07:14:41.379180Z","iopub.execute_input":"2025-02-13T07:14:41.379493Z","iopub.status.idle":"2025-02-13T07:16:44.086233Z","shell.execute_reply.started":"2025-02-13T07:14:41.379467Z","shell.execute_reply":"2025-02-13T07:16:44.083230Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Cleaning & Memory Optimization\n","metadata":{}},{"cell_type":"code","source":"# ------------------------------\n# Memory Reduction Function\n# ------------------------------\ndef reduce_mem_usage(df):\n    \"\"\" Reduce memory usage of a DataFrame by downcasting numerical columns. \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    for col in df.columns:\n        col_type = df[col].dtype\n        if col_type not in [object, 'category', 'datetime64[ns]']:\n            c_min, c_max = df[col].min(), df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                else:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                df[col] = df[col].astype(np.float32)\n    end_mem = df.memory_usage().sum() / 1024**2\n    print(f\"Memory usage reduced from {start_mem:.2f} MB to {end_mem:.2f} MB\")\n    return df\n\n# Reduce memory usage of dataset\ntrain_merged_corrected = reduce_mem_usage(train_merged_corrected)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ------------------------------\n# Data Cleaning & Memory Optimization\n# ------------------------------\n# Drop unnecessary columns to reduce dataset size\ncolumns_to_drop = ['unique_id', 'date']\ntrain_merged_corrected = train_merged_corrected.drop(columns=[\n    'is_weekend','day_sin','day_cos','sales_max','sales_mean','sales_median','sales_std',\n    'sales_skew','sales_zero_ratio','total_orders_max','total_orders_mean','total_orders_median',\n    'total_orders_std','total_orders_skew','total_orders_zero_ratio','sell_price_main_max',\n    'sell_price_main_mean','sell_price_main_median','sell_price_main_std','sell_price_main_skew',\n    'sell_price_main_zero_ratio','total_discount_max','total_discount_mean','total_discount_median',\n    'total_discount_std','total_discount_skew','total_discount_zero_ratio','sales_yearly_q25',\n    'sales_yearly_q75','total_orders_yearly_q25','total_orders_yearly_q75','total_orders_yearly_zero_ratio',\n    'total_orders_yearly_cv','sell_price_main_yearly_q25','sell_price_main_yearly_q75',\n    'sell_price_main_yearly_zero_ratio','sell_price_main_yearly_cv','availability_yearly_max',\n    'total_discount_yearly_q25','total_discount_yearly_q75','total_discount_yearly_zero_ratio',\n    'days_to_next_closed', 'total_discount_yearly_min'\n])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T07:17:25.200165Z","iopub.execute_input":"2025-02-13T07:17:25.200520Z","iopub.status.idle":"2025-02-13T07:17:25.310549Z","shell.execute_reply.started":"2025-02-13T07:17:25.200494Z","shell.execute_reply":"2025-02-13T07:17:25.309481Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\ndef reduce_mem_usage(df):\n    start_mem = df.memory_usage().sum() / 1024**2\n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type not in [object, 'category', 'datetime64[ns]']:\n            c_min = df[col].min()\n            c_max = df[col].max()\n\n            # tamsayı sütunlar\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                else:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                # float sütunlar\n                df[col] = df[col].astype(np.float32)\n                \n        elif col_type == object:\n            # Eğer gerçekte kategorik veya sayısal değilse, kategorik dönüştürebilirsiniz\n            # df[col] = df[col].astype('category')\n            pass\n    \n    end_mem = df.memory_usage().sum() / 1024**2\n    print(f\"Bellek kullanımı: {start_mem:.2f} MB -> {end_mem:.2f} MB\")\n    return df\ntrain_merged_corrected=  reduce_mem_usage  (train_merged_corrected)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T07:17:30.443790Z","iopub.execute_input":"2025-02-13T07:17:30.444207Z","iopub.status.idle":"2025-02-13T07:17:34.230994Z","shell.execute_reply.started":"2025-02-13T07:17:30.444178Z","shell.execute_reply":"2025-02-13T07:17:34.229679Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Feature Engineering & Data Splitting","metadata":{}},{"cell_type":"code","source":"# Convert the 'date' column to datetime format and sort the DataFrame by date\ntrain_merged_corrected['date'] = pd.to_datetime(train_merged_corrected['date'])\ntrain_merged_corrected = train_merged_corrected.sort_values('date')\n\n# Drop the defined columns to simplify the dataset\ndf = train_merged_corrected.drop(columns=columns_to_drop)\n\n# Select non-numeric and non-string columns to identify categorical features\nnon_num_str_df = df.select_dtypes(exclude=[\"number\", \"string\"])\ndel df  # Delete temporary DataFrame to free memory\n\n# List out the identified categorical features\ncategorical_features = non_num_str_df.columns.tolist()\ncategorical_features.append('product_unique_id')\n\n# Convert the identified categorical columns in the original DataFrame to string type\ntrain_merged_corrected[categorical_features] = train_merged_corrected[categorical_features].astype('string')\nfor col in categorical_features:\n    # Ensure that missing values are filled with a placeholder (\"-1\") and cast as string\n    train_merged_corrected[col] = train_merged_corrected[col].astype(str).fillna(\"-1\")\n\n# =============================================================================\n# Data Splitting: Define Sales Train and Test Sets Based on Date Ranges\n# =============================================================================\n\n# Define the training period for sales data: 2020-08-01 to 2024-06-02\nsales_train = train_merged_corrected[\n    (train_merged_corrected['date'] >= pd.Timestamp('2020-08-01')) &\n    (train_merged_corrected['date'] <= pd.Timestamp('2024-06-02'))\n]\n\n# Define the test period for sales data: 2024-06-03 to 2024-06-16\nsales_test = train_merged_corrected[\n    (train_merged_corrected['date'] >= pd.Timestamp('2024-06-03')) &\n    (train_merged_corrected['date'] <= pd.Timestamp('2024-06-16'))\n]\ndel non_num_str_df\ndel train_merged_corrected\ngc.collect()  # Clean up memory\n\n\n\n\n# Split the training data into training and validation sets without shuffling \n# to maintain the time series order\nX_train, X_val, y_train, y_val = train_test_split(\n    sales_train.drop(columns=columns_to_drop + ['sales']), \n    sales_train['sales'], \n    test_size=0.1, \n    shuffle=False\n)\n\n\n# Prepare the test set by dropping the columns not needed\nX_test = sales_test.drop(columns=columns_to_drop)\n\n\n\n\n# =============================================================================\n# Handle Sample Weights and Memory Reduction for Training, Validation, and Test Sets\n# =============================================================================\n\n# Save the weight column separately for training, validation, and test sets\ntrain_weight = X_train['weight']\nval_weight   = X_val['weight']\ntest_weight  = X_test['weight']\n\n# Apply memory reduction on each set\nX_train = reduce_mem_usage(X_train)\nX_val   = reduce_mem_usage(X_val)\nX_test  = reduce_mem_usage(X_test)\n\n# Drop the 'weight' column from feature sets as it is stored separately\nX_train = X_train.drop(['weight'], axis=1)\nX_val   = X_val.drop(['weight'], axis=1)\nX_test  = X_test.drop(['weight'], axis=1)\n\n# =============================================================================\n# Process Categorical Features: Ensure all categorical features are strings\n# =============================================================================\n\n# Convert categorical features to string type for training, validation, and test sets\nX_train[categorical_features] = X_train[categorical_features].astype(str)\nX_val[categorical_features]   = X_val[categorical_features].astype(str)\nX_test[categorical_features]  = X_test[categorical_features].astype(str)\n\n# Debug: Print data type and unique values of each categorical feature in validation and test sets\nfor col in categorical_features:\n    print(col, X_val[col].dtype, X_test[col].unique())\n\n# Set option to display all items in sequences for better debugging/inspection\npd.set_option('display.max_seq_items', None)\n\nprint(\"Those are categorical features:\")\nprint(categorical_features)\n\n# Loop through each categorical feature and print its data type and unique values\nfor col in categorical_features:\n    print(col, X_val[col].dtype)\n    print(\"X_val unique values:\")\n    print(X_val[col].unique())\n    print(\"X_test unique values:\")\n    print(X_test[col].unique())\n    print(\"\\n\" + \"=\"*50 + \"\\n\")\n\n# =============================================================================\n# Log Transformation on the Target Variable\n# =============================================================================\n\n# Apply log1p transformation to the target variable for better model stability\ny_train_log = np.log1p(y_train)\ny_val_log   = np.log1p(y_val)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T07:16:44.194396Z","iopub.status.idle":"2025-02-13T07:16:44.194925Z","shell.execute_reply":"2025-02-13T07:16:44.194707Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Training & Feature Importance Analysis","metadata":{}},{"cell_type":"code","source":"from catboost import CatBoostRegressor\nfrom sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit, train_test_split\nfrom sklearn.metrics import mean_absolute_error\nimport matplotlib.pyplot as plt\nfrom catboost import Pool\n# Best parameters for the CatBoostRegressor model\nbest_params = {\n    'eval_metric': 'MAE',\n    'bagging_temperature': 0.25,\n    'iterations': 2500,\n    'learning_rate': 0.05,\n    'max_depth': 10,\n    'l2_leaf_reg': 1,\n    'min_data_in_leaf': 24,\n    'random_strength': 0.25\n}\n\n# List the names of all global variables that are pandas DataFrames for debugging purposes\ndfs_in_memory = [\n    var_name \n    for var_name, var_value in globals().items() \n    if isinstance(var_value, pd.DataFrame)\n]\ngc.collect()  # Clean up memory\nprint(\"Aşağıdaki isimler pandas DataFrame nesnelerini temsil ediyor:\")\nprint(dfs_in_memory)\n\n# Create an evaluation pool for CatBoost using the validation set\neval_pool = Pool(data=X_val, label=y_val_log, cat_features=categorical_features, weight=val_weight)\n\n# Initialize the CatBoostRegressor with specified parameters, using GPU acceleration\nbest_model = CatBoostRegressor(\n    **best_params, \n    loss_function='MAE', \n    random_seed=42,\n    boosting_type='Ordered',\n    thread_count=-1,      # Use all available CPU cores\n    task_type='GPU',\n    allow_writing_files=False,\n    max_bin=128\n)\n\n# Train the model using the training data, with early stopping and evaluation on the validation set\nbest_model.fit(\n    X_train, y_train_log,\n    sample_weight=train_weight,\n    eval_set=eval_pool,\n    cat_features=categorical_features,\n    verbose=300,\n    early_stopping_rounds=10\n)\n\n# =============================================================================\n# Predictions: Validate and Generate Test Set Predictions\n# =============================================================================\n\n# Predict on the validation set (output is in log scale)\ny_val_pred_log = best_model.predict(X_val)\n\n# Create a prediction pool for the test set\ntest_pool = Pool(data=X_test, cat_features=categorical_features, weight=test_weight)\n# Predict on the test set (output is in log scale)\ny_test_pred_log = best_model.predict(test_pool)\n\n# Convert predictions back from log scale to original scale using expm1 (inverse of log1p)\ny_val_pred = np.expm1(y_val_pred_log)\ny_test_pred = np.expm1(y_test_pred_log)\n\n# Delete the temporary log-scale predictions to free memory\ndel y_val_pred_log\ndel y_test_pred_log\ngc.collect()\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Results","metadata":{}},{"cell_type":"code","source":"# =============================================================================\n# Post-processing: Adjust Predictions Based on Thresholds\n# =============================================================================\n\n# Adjust predictions if they exceed a threshold of 5000 (scaling factor 1.1)\ny_val_pred2 = np.where(y_val_pred > 5000, y_val_pred * 1.1, y_val_pred)\ny_test_pred2 = np.where(y_test_pred > 5000, y_test_pred * 1.1, y_test_pred)\n\n# Another adjustment for predictions exceeding 6000 (scaling factor 1.1)\ny_val_pred3 = np.where(y_val_pred > 6000, y_val_pred * 1.1, y_val_pred)\ny_test_pred3 = np.where(y_test_pred > 6000, y_test_pred * 1.1, y_test_pred)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# =============================================================================\n# Feature Importance Reporting and Export\n# =============================================================================\n\n# Calculate feature importances from the trained CatBoost model using the training data\nfeature_importances = best_model.get_feature_importance(Pool(X_train, cat_features=categorical_features))\nfeature_names = X_train.columns.tolist()\n\n# Create a DataFrame to store feature names and their corresponding importance values\nfeature_importances_df = pd.DataFrame({\n    \"Feature\": feature_names,\n    \"Importance\": feature_importances\n})\n\n# Sort the DataFrame by importance in descending order\nfeature_importances_df = feature_importances_df.sort_values(by=\"Importance\", ascending=False)\n\n# Print the feature importances to the console\nprint(\"Feature Importances:\")\nprint(feature_importances_df)\n\n\n# Plot the feature importances using a horizontal bar chart\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 12))\nplt.barh(feature_importances_df[\"Feature\"], feature_importances_df[\"Importance\"])\nplt.xlabel(\"Feature Importance\")\nplt.ylabel(\"Feature\")\nplt.title(\"Feature Importances from CatBoost Model\")\nplt.gca().invert_yaxis()  # Invert y-axis so the most important features are at the top\nplt.tight_layout()\nplt.show()\n\n# Export the feature importances to a CSV file\nfeature_importances_df.to_csv(\"feature_importances.csv\", index=False)\nprint(\"Feature importances exported to 'feature_importances.csv'\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# =============================================================================\n# Model Evaluation: Calculate MAE and Weighted MAE Metrics\n# =============================================================================\n\n# Calculate Mean Absolute Error (MAE) on the validation set\nmae_val = mean_absolute_error(y_val, y_val_pred)\nprint(f\"Test MAE VAL: {mae_val}\")\n\n# Calculate Weighted MAE using the adjusted predictions (first adjustment with threshold 5000)\nweighted_mae_val = np.sum(val_weight * np.abs(y_val - y_val_pred2)) / np.sum(val_weight)\nprint(f\"Weighted MAE VAL: {weighted_mae_val}\")\n\n# Calculate Weighted MAE using the adjusted predictions (second adjustment with threshold 6000)\nweighted_mae_val = np.sum(val_weight * np.abs(y_val - y_val_pred3)) / np.sum(val_weight)\nprint(f\"Weighted MAE VAL: {weighted_mae_val}\")\n\n# Calculate Weighted MAE using the original predictions without adjustments\nweighted_mae_val = np.sum(val_weight * np.abs(y_val - y_val_pred)) / np.sum(val_weight)\nprint(f\"Weighted MAE VAL: {weighted_mae_val}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================================================================\n# Visualization: Scatter Plots for Actual vs Predicted Sales\n# =============================================================================\n\nimport matplotlib.pyplot as plt\n\n# Plot actual sales vs predicted sales (original predictions)\nplt.figure(figsize=(10, 6))\nplt.scatter(y_val, y_val_pred, alpha=0.5)\nplt.title(\"Actual vs Predicted Sales\")\nplt.xlabel(\"Actual Sales\")\nplt.ylabel(\"Predicted Sales\")\nplt.grid(True)\nplt.show()\n\n# Plot actual sales vs predicted sales (first adjustment)\nplt.figure(figsize=(10, 6))\nplt.scatter(y_val, y_val_pred2, alpha=0.5)\nplt.title(\"Actual vs Predicted Sales\")\nplt.xlabel(\"Actual Sales\")\nplt.ylabel(\"Predicted Sales\")\nplt.grid(True)\nplt.show()\n\n# Plot actual sales vs predicted sales (second adjustment)\nplt.figure(figsize=(10, 6))\nplt.scatter(y_val, y_val_pred3, alpha=0.5)\nplt.title(\"Actual vs Predicted Sales\")\nplt.xlabel(\"Actual Sales\")\nplt.ylabel(\"Predicted Sales\")\nplt.grid(True)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================================================================\n# Visualization: Residual Plots\n# =============================================================================\n\n# Calculate residuals for the original predictions (difference between actual and predicted values)\nresiduals = y_val - y_val_pred\n\n# Plot residuals for the original predictions\nplt.figure(figsize=(10, 6))\nplt.scatter(y_val_pred, residuals, alpha=0.6)\nplt.axhline(y=0, color='red', linestyle='--', linewidth=2)  # Add horizontal zero line for reference\nplt.xlabel(\"Tahmin Edilen Değerler (Sales)\")\nplt.ylabel(\"Residual (Gerçek - Tahmin)\")\nplt.title(\"Residual Grafiği\")\nplt.show()\n\n# Calculate residuals for the first adjustment predictions\nresiduals = y_val - y_val_pred2\n\n# Plot residuals for the first adjustment predictions\nplt.figure(figsize=(10, 6))\nplt.scatter(y_val_pred, residuals, alpha=0.6)\nplt.axhline(y=0, color='red', linestyle='--', linewidth=2)\nplt.xlabel(\"Tahmin Edilen Değerler (Sales)\")\nplt.ylabel(\"Residual (Gerçek - Tahmin)\")\nplt.title(\"Residual Grafiği\")\nplt.show()\n\n# Calculate residuals for the second adjustment predictions\nresiduals = y_val - y_val_pred3\n\n# Plot residuals for the second adjustment predictions\nplt.figure(figsize=(10, 6))\nplt.scatter(y_val_pred, residuals, alpha=0.6)\nplt.axhline(y=0, color='red', linestyle='--', linewidth=2)\nplt.xlabel(\"Tahmin Edilen Değerler (Sales)\")\nplt.ylabel(\"Residual (Gerçek - Tahmin)\")\nplt.title(\"Residual Grafiği\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================================================================\n# Post-processing for Test Set Predictions and Submission File Creation\n# =============================================================================\n\n# Ensure that no negative predictions exist by setting negatives to 0\ny_test_pred[y_test_pred < 0] = 0\n\n# Create a unique submission ID by concatenating 'unique_id' and 'date'\nsales_test['id'] = sales_test['unique_id'].astype(str) + \"_\" + sales_test['date'].astype(str)\n# Assign the predicted sales values to a new column 'sales_hat'\nsales_test['sales_hat'] = y_test_pred\n# Export the predictions to a CSV file for submission\nsales_test[['id', 'sales_hat']].to_csv(\"submission.csv\", index=False)\n\n# Second submission: Apply the first adjustment to ensure negatives are handled (if any)\ny_test_pred[y_test_pred2 < 0] = 0\nsales_test['id'] = sales_test['unique_id'].astype(str) + \"_\" + sales_test['date'].astype(str)\nsales_test['sales_hat'] = y_test_pred\nsales_test[['id', 'sales_hat']].to_csv(\"submission2.csv\", index=False)\n\n# Third submission: Apply the second adjustment to ensure negatives are handled (if any)\ny_test_pred[y_test_pred3 < 0] = 0\nsales_test['id'] = sales_test['unique_id'].astype(str) + \"_\" + sales_test['date'].astype(str)\nsales_test['sales_hat'] = y_test_pred\nsales_test[['id', 'sales_hat']].to_csv(\"submission3.csv\", index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}